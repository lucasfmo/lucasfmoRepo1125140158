---
title: Clasificación de imágenes con CNTK en Azure Machine Learning Workbench | Microsoft Docs
description: Use Azure ML Workbench para probar, evaluar e implementar un modelo personalizado de clasificación de imágenes.
services: machine-learning
documentationcenter: ''
author: PatrickBue
ms.author: pabuehle
manager: mwinkle
ms.reviewer: marhamil, mldocs, garyericson, jasonwhowell
ms.service: machine-learning
ms.workload: data-services
ms.topic: article
ms.date: 10/17/2017
ms.openlocfilehash: 8bf5cd802198cba48a99c029d0c75c25dd5f6d84
ms.sourcegitcommit: 59914a06e1f337399e4db3c6f3bc15c573079832
ms.contentlocale: es-ES
ms.lasthandoff: 04/19/2018
---
# <a name="image-classification-using-azure-machine-learning-workbench"></a><span data-ttu-id="f4842-103">Clasificación de imágenes con Azure Machine Learning Workbench</span><span class="sxs-lookup"><span data-stu-id="f4842-103">Image classification using Azure Machine Learning Workbench</span></span>

<span data-ttu-id="f4842-104">Se pueden usar distintos métodos de clasificación de imágenes para resolver un gran número de problemas de Computer Vision.</span><span class="sxs-lookup"><span data-stu-id="f4842-104">Image classification approaches can be used to solve a large number of Computer Vision problems.</span></span>
<span data-ttu-id="f4842-105">Esto incluye la creación de modelos que respondan preguntas del tipo *¿Hay un OBJETO en la imagen?*, donde OBJETO puede ser, por ejemplo, un *perro*, un *coche* o un *barco*.</span><span class="sxs-lookup"><span data-stu-id="f4842-105">These include building models, which answer questions such as: *Is an OBJECT present in the image?* where OBJECT could for example be *dog*, *car*, or *ship*.</span></span> <span data-ttu-id="f4842-106">O a preguntas más complejas, como *¿Qué gravedad de enfermedad ocular revela el escáner de retina de este paciente?*</span><span class="sxs-lookup"><span data-stu-id="f4842-106">Or more complex questions like: *What class of eye disease severity is evinced by this patient's retinal scan?*.</span></span>

<span data-ttu-id="f4842-107">En este tutorial se abordan estos problemas.</span><span class="sxs-lookup"><span data-stu-id="f4842-107">This tutorial addresses solving such problems.</span></span> <span data-ttu-id="f4842-108">En él explicamos cómo entrenar, evaluar e implementar su propio modelo de clasificación de imágenes usando [Microsoft Cognitive Toolkit (CNTK)](https://docs.microsoft.com/cognitive-toolkit/) para el aprendizaje profundo.</span><span class="sxs-lookup"><span data-stu-id="f4842-108">We show how to train, evaluate, and deploy your own image classification model using the  [Microsoft Cognitive Toolkit (CNTK) ](https://docs.microsoft.com/cognitive-toolkit/) for deep learning.</span></span>
<span data-ttu-id="f4842-109">Se incluyen imágenes de ejemplo, si bien el lector puede aportar su propio conjunto de datos y entrenar modelos personalizados de su cosecha.</span><span class="sxs-lookup"><span data-stu-id="f4842-109">Example images are provided, but the reader can also bring their own dataset and train their own custom models.</span></span>

<span data-ttu-id="f4842-110">Las soluciones de Computer Vision solían requerir un conocimiento de experto para identificar e implementar manualmente las llamadas *características*, que resaltan la información deseada en imágenes.</span><span class="sxs-lookup"><span data-stu-id="f4842-110">Computer Vision solutions traditionally required expert knowledge to manually identify and implement so-called *features*, which highlight desired information in images.</span></span>
<span data-ttu-id="f4842-111">Este método manual cambió en 2012 con el famoso artículo de [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) [1] sobre aprendizaje profundo y, en la actualidad, se usan redes neuronales profundas (DNN) para encontrar estas características automáticamente.</span><span class="sxs-lookup"><span data-stu-id="f4842-111">This manual approach changed in 2012 with the famous [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) [1] Deep Learning paper, and at present, Deep Neural Networks (DNN) are used to automatically find these features.</span></span>
<span data-ttu-id="f4842-112">Las DNN supusieron una notable mejora en el campo no ya de la clasificación de imágenes, sino también para acabar con diversos problemas de Computer Vision como la detección de objetos y la similitud de imágenes.</span><span class="sxs-lookup"><span data-stu-id="f4842-112">DNNs led to a huge improvement in the field not just for image classification, but also for other Computer Vision problems such as object detection and image similarity.</span></span>


## <a name="link-to-the-gallery-github-repository"></a><span data-ttu-id="f4842-113">Vínculo al repositorio de GitHub de la galería</span><span class="sxs-lookup"><span data-stu-id="f4842-113">Link to the gallery GitHub repository</span></span>
[https://github.com/Azure/MachineLearningSamples-ImageClassificationUsingCNTK](https://github.com/Azure/MachineLearningSamples-ImageClassificationUsingCNTK)

## <a name="overview"></a><span data-ttu-id="f4842-114">Información general</span><span class="sxs-lookup"><span data-stu-id="f4842-114">Overview</span></span>

<span data-ttu-id="f4842-115">Este tutorial se divide en tres partes:</span><span class="sxs-lookup"><span data-stu-id="f4842-115">This tutorial is split into three parts:</span></span>

- <span data-ttu-id="f4842-116">En la parte 1 se muestra cómo entrenar, evaluar e implementar un sistema de clasificación de imágenes, usando para ello una DNN ya entrenada para la captura de características y probando un SVM en la salida.</span><span class="sxs-lookup"><span data-stu-id="f4842-116">Part 1 shows how to train, evaluate, and deploy an image classification system using a pre-trained DNN as featurizer and training an SVM on its output.</span></span>
- <span data-ttu-id="f4842-117">En la parte 2 siguiente se indica cómo mejorar la precisión, por ejemplo, perfeccionando la DNN en lugar de usarla para la captura de características.</span><span class="sxs-lookup"><span data-stu-id="f4842-117">Part 2 then shows how to improve accuracy by, for example, refining the DNN rather than using it as a fixed featurizer.</span></span>
- <span data-ttu-id="f4842-118">En la parte 3 se describe cómo puede usar un conjunto de datos de su cosecha en lugar de las imágenes de ejemplo suministradas y, si es necesario, cómo generar su propio conjunto de datos extrayendo imágenes de Internet.</span><span class="sxs-lookup"><span data-stu-id="f4842-118">Part 3 covers how to use your own dataset instead of the provided example images, and if needed, how to produce your own dataset by scraping images from the net.</span></span>

<span data-ttu-id="f4842-119">No se requiere experiencia previa de aprendizaje automático y CNTK, pero sí resulta útil para comprender los principios subyacentes.</span><span class="sxs-lookup"><span data-stu-id="f4842-119">While previous experience with machine learning and CNTK is not required, it is helpful for understanding the underlying principles.</span></span> <span data-ttu-id="f4842-120">La precisión de los números, la duración del entrenamiento y otras cuestiones reflejadas en el tutorial se incluyen exclusivamente a modo de referencia; de igual modo, los valores reales que se obtengan al ejecutar el código variarán casi con toda seguridad.</span><span class="sxs-lookup"><span data-stu-id="f4842-120">Accuracy numbers, training time, etc. reported in the tutorial are only for reference, and the actual values when running the code almost certainly differ.</span></span>


## <a name="prerequisites"></a><span data-ttu-id="f4842-121">requisitos previos</span><span class="sxs-lookup"><span data-stu-id="f4842-121">Prerequisites</span></span>

<span data-ttu-id="f4842-122">Los requisitos previos para ejecutar este ejemplo son los siguientes:</span><span class="sxs-lookup"><span data-stu-id="f4842-122">The prerequisites to run this example are as follows:</span></span>

1. <span data-ttu-id="f4842-123">Una [cuenta de Azure](https://azure.microsoft.com/free/) (hay disponibles versiones gratuitas de prueba).</span><span class="sxs-lookup"><span data-stu-id="f4842-123">An [Azure account](https://azure.microsoft.com/free/) (free trials are available).</span></span>
2. <span data-ttu-id="f4842-124">Una instalación de [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md) siguiendo la [Guía de instalación de inicio rápido](../service/quickstart-installation.md) para instalar el programa y crear un área de trabajo.</span><span class="sxs-lookup"><span data-stu-id="f4842-124">The [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md) following the [quick start installation guide](../service/quickstart-installation.md) to install the program and create a workspace.</span></span>  
3. <span data-ttu-id="f4842-125">Un equipo Windows.</span><span class="sxs-lookup"><span data-stu-id="f4842-125">A Windows machine.</span></span> <span data-ttu-id="f4842-126">Se necesita un sistema operativo Windows porque Workbench admite únicamente Windows y Mac OS, mientras que Microsoft Cognitive Toolkit (que usaremos como biblioteca de aprendizaje profundo) solo es compatible con Windows y Linux.</span><span class="sxs-lookup"><span data-stu-id="f4842-126">Windows OS is necessary since the Workbench supports only Windows and MacOS, while Microsoft's Cognitive Toolkit (which we use as deep learning library) only supports Windows and Linux.</span></span>
4. <span data-ttu-id="f4842-127">No se necesita una GPU dedicada para ejecutar el entrenamiento de SVM en la parte 1, pero sí para el proceso de perfeccionamiento de la DNN descrito en la parte 2.</span><span class="sxs-lookup"><span data-stu-id="f4842-127">A dedicated GPU is not required to execute the SVM training in part 1, however it is needed for refining of the DNN described in part 2.</span></span> <span data-ttu-id="f4842-128">Si no tiene una GPU en condiciones, quiere entrenar en varias GPU o no tiene un equipo con Windows, considere la posibilidad de usar máquina de virtual de aprendizaje profundo de Azure con un sistema operativo Windows.</span><span class="sxs-lookup"><span data-stu-id="f4842-128">If you lack a strong GPU, want to train on multiple GPUs, or do not have a Windows machine, then consider using Azure's Deep Learning Virtual Machine with Windows operating system.</span></span> <span data-ttu-id="f4842-129">Vaya [aquí](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-ads.dsvm-deep-learning) para obtener una guía de implementación con un solo clic.</span><span class="sxs-lookup"><span data-stu-id="f4842-129">See [here](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-ads.dsvm-deep-learning) for a 1-click deployment guide.</span></span> <span data-ttu-id="f4842-130">Una vez implementada, conéctese a la máquina virtual a través de una conexión de Escritorio remoto, instale Workbench y ejecute el código localmente en ella.</span><span class="sxs-lookup"><span data-stu-id="f4842-130">Once deployed, connect to the VM via a remote desktop connection, install Workbench there, and execute the code locally from the VM.</span></span>
5. <span data-ttu-id="f4842-131">Es necesario instalar varias bibliotecas de Python, como OpenCV.</span><span class="sxs-lookup"><span data-stu-id="f4842-131">Various Python libraries such as OpenCV need to be installed.</span></span> <span data-ttu-id="f4842-132">Haga clic en *Abrir símbolo del sistema* en el menú *Archivo* de Workbench y ejecute los siguientes comandos para instalar estas dependencias:</span><span class="sxs-lookup"><span data-stu-id="f4842-132">Click *Open Command Prompt* from the *File* menu in the Workbench and run the following commands to install these dependencies:</span></span>  
    - `pip install https://cntk.ai/PythonWheel/GPU/cntk-2.2-cp35-cp35m-win_amd64.whl`  
    - <span data-ttu-id="f4842-133">`pip install opencv_python-3.3.1-cp35-cp35m-win_amd64.whl` después de descargar OpenCV de http://www.lfd.uci.edu/~gohlke/pythonlibs/ (la versión y el nombre de archivo exactos pueden variar).</span><span class="sxs-lookup"><span data-stu-id="f4842-133">`pip install opencv_python-3.3.1-cp35-cp35m-win_amd64.whl` after downloading the OpenCV wheel from http://www.lfd.uci.edu/~gohlke/pythonlibs/ (the exact filename and version can change)</span></span>
    - `conda install pillow`
    - `pip install -U numpy`
    - `pip install bqplot`
    - `jupyter nbextension enable --py --sys-prefix bqplot`
    - `jupyter nbextension enable --py widgetsnbextension`

### <a name="troubleshooting--known-bugs"></a><span data-ttu-id="f4842-134">Solución de problemas/Errores conocidos</span><span class="sxs-lookup"><span data-stu-id="f4842-134">Troubleshooting / Known bugs</span></span>
- <span data-ttu-id="f4842-135">Se necesita una GPU para la parte 2. Si no hay una, cuando trate de perfeccionar la DNN aparecerá un error que indica que el aprendizaje de normalización por lotes todavía no se ha implementado.</span><span class="sxs-lookup"><span data-stu-id="f4842-135">A GPU is needed for part 2, and otherwise the error "Batch normalization training on CPU is not yet implemented" is thrown when trying to refine the DNN.</span></span>
- <span data-ttu-id="f4842-136">Los errores de memoria insuficiente durante el entrenamiento de DNN se pueden soslayar si se reduce el tamaño del minilote (variable `cntk_mb_size` en `PARAMETERS.py`).</span><span class="sxs-lookup"><span data-stu-id="f4842-136">Out-of-memory errors during DNN training can be avoided by reducing the minibatch size (variable `cntk_mb_size` in `PARAMETERS.py`).</span></span>
- <span data-ttu-id="f4842-137">El código se probó con CNTK 2.2 y debería poder ejecutarse tanto en versiones anteriores (hasta 2.0) como en versiones más recientes sin cambios (o solo con cambios mínimos).</span><span class="sxs-lookup"><span data-stu-id="f4842-137">The code was tested using CNTK 2.2, and should run also on older (up to v2.0) and newer versions without any or only minor changes.</span></span>
- <span data-ttu-id="f4842-138">En el momento de redactar este artículo, Azure Machine Learning Workbench generaba problemas a la hora de mostrar blocs de notas superiores a 5 MB.</span><span class="sxs-lookup"><span data-stu-id="f4842-138">At the time of writing, the Azure Machine Learning Workbench had problems showing notebooks larger than 5 Mbytes.</span></span> <span data-ttu-id="f4842-139">Puede haber blocs de notas con este tamaño si se guardan mostrando todas las salidas de celda.</span><span class="sxs-lookup"><span data-stu-id="f4842-139">Notebooks of this large size can happen if the notebook is saved with all cell output displayed.</span></span> <span data-ttu-id="f4842-140">Si este error se produce, abra un símbolo del sistema en el menú Archivo de Workbench, ejecute `jupyter notebook`, abra el bloc de notas, borre todas las salidas y guárdelo.</span><span class="sxs-lookup"><span data-stu-id="f4842-140">If you encounter this error, then open the command prompt from the File menu inside the Workbench, execute `jupyter notebook`, open the notebook, clear all output, and save the notebook.</span></span> <span data-ttu-id="f4842-141">Una vez hecho esto, el bloc de notas volverá a abrirse correctamente en Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="f4842-141">After performing these steps, the notebook will open properly inside the Azure Machine Learning Workbench again.</span></span>
- <span data-ttu-id="f4842-142">Todos los scripts que se proporcionan en este ejemplo tienen que ejecutarse de forma local y no en entornos como, por ejemplo, un entorno remoto de docker.</span><span class="sxs-lookup"><span data-stu-id="f4842-142">All scripts provided in this sample have to be executed locally, and not on e.g. a docker remote environment.</span></span> <span data-ttu-id="f4842-143">Todos los blocs de notas tienen que ejecutarse con el kernel establecido como un kernel del proyecto local denominado "PROJECTNAME local" (por ejemplo, "myImgClassUsingCNTK local").</span><span class="sxs-lookup"><span data-stu-id="f4842-143">All notebooks need to be executed with kernel set to the local project kernel with name "PROJECTNAME local" (e.g. "myImgClassUsingCNTK local").</span></span>

    
## <a name="create-a-new-workbench-project"></a><span data-ttu-id="f4842-144">Creación de un nuevo proyecto de Workbench</span><span class="sxs-lookup"><span data-stu-id="f4842-144">Create a new workbench project</span></span>

<span data-ttu-id="f4842-145">Para crear un proyecto usando este ejemplo como plantilla:</span><span class="sxs-lookup"><span data-stu-id="f4842-145">To create a new project using this example as a template:</span></span>
1.  <span data-ttu-id="f4842-146">Abra Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="f4842-146">Open Azure Machine Learning Workbench.</span></span>
2.  <span data-ttu-id="f4842-147">En la página **Proyectos**, haga clic en el signo **+** y seleccione **Nuevo proyecto**.</span><span class="sxs-lookup"><span data-stu-id="f4842-147">On the **Projects** page, click the **+** sign and select **New Project**.</span></span>
3.  <span data-ttu-id="f4842-148">En el panel **Crear nuevo proyecto**, rellene la información del proyecto nuevo.</span><span class="sxs-lookup"><span data-stu-id="f4842-148">In the **Create New Project** pane, fill in the information for your new project.</span></span>
4.  <span data-ttu-id="f4842-149">En el cuadro de búsqueda **Buscar plantillas de proyecto**, escriba "Image Classification" (Clasificación de imágenes) y seleccione la plantilla.</span><span class="sxs-lookup"><span data-stu-id="f4842-149">In the **Search Project Templates** search box, type "Image classification" and select the template.</span></span>
5.  <span data-ttu-id="f4842-150">Haga clic en **Create**(Crear).</span><span class="sxs-lookup"><span data-stu-id="f4842-150">Click **Create**.</span></span>

<span data-ttu-id="f4842-151">Cuando realice estos pasos, se creará la estructura del proyecto descrita abajo.</span><span class="sxs-lookup"><span data-stu-id="f4842-151">Performing these steps creates the project structure shown below.</span></span> <span data-ttu-id="f4842-152">El directorio del proyecto tiene una limitación de tamaño máximo de 25 MB, ya que Azure Machine Learning Workbench crea una copia de esta carpeta después de cada ejecución (para habilitar el historial de ejecución).</span><span class="sxs-lookup"><span data-stu-id="f4842-152">The project directory is restricted to be less than 25 Mbytes since the Azure Machine Learning Workbench creates a copy of this folder after each run (to enable run history).</span></span> <span data-ttu-id="f4842-153">Por lo tanto, todos los archivos temporales y de imagen se guardan en el directorio *~/Desktop/imgClassificationUsingCntk_data* (en este documento lo llamaremos *DATA_DIR*).</span><span class="sxs-lookup"><span data-stu-id="f4842-153">Hence, all image and temporary files are saved to and from the directory *~/Desktop/imgClassificationUsingCntk_data* (referred to as *DATA_DIR* in this document).</span></span>

  <span data-ttu-id="f4842-154">Carpeta</span><span class="sxs-lookup"><span data-stu-id="f4842-154">Folder</span></span>| <span data-ttu-id="f4842-155">DESCRIPCIÓN</span><span class="sxs-lookup"><span data-stu-id="f4842-155">Description</span></span>
  ---|---
  <span data-ttu-id="f4842-156">aml_config/</span><span class="sxs-lookup"><span data-stu-id="f4842-156">aml_config/</span></span>|                           <span data-ttu-id="f4842-157">Directorio que contiene los archivos de configuración de Azure Machine Learning Workbench</span><span class="sxs-lookup"><span data-stu-id="f4842-157">Directory containing the Azure Machine Learning Workbench configuration files</span></span>
  <span data-ttu-id="f4842-158">libraries/</span><span class="sxs-lookup"><span data-stu-id="f4842-158">libraries/</span></span>|                              <span data-ttu-id="f4842-159">Directorio que contiene todas las funciones auxiliares de Python y Jupyter</span><span class="sxs-lookup"><span data-stu-id="f4842-159">Directory containing all Python and Jupyter helper functions</span></span>
  <span data-ttu-id="f4842-160">notebooks/</span><span class="sxs-lookup"><span data-stu-id="f4842-160">notebooks/</span></span>|                              <span data-ttu-id="f4842-161">Directorio que contiene todos los blocs de notas</span><span class="sxs-lookup"><span data-stu-id="f4842-161">Directory containing all notebooks</span></span>
  <span data-ttu-id="f4842-162">resources/</span><span class="sxs-lookup"><span data-stu-id="f4842-162">resources/</span></span>|                              <span data-ttu-id="f4842-163">Directorio que contiene todos los recursos (por ejemplo, direcciones URL de imágenes de moda)</span><span class="sxs-lookup"><span data-stu-id="f4842-163">Directory containing all resources (for example url of fashion images)</span></span>
  <span data-ttu-id="f4842-164">scripts/</span><span class="sxs-lookup"><span data-stu-id="f4842-164">scripts/</span></span>|                              <span data-ttu-id="f4842-165">Directorio que contiene todos los scripts</span><span class="sxs-lookup"><span data-stu-id="f4842-165">Directory containing all scripts</span></span>
  <span data-ttu-id="f4842-166">PARAMETERS.py</span><span class="sxs-lookup"><span data-stu-id="f4842-166">PARAMETERS.py</span></span>|                       <span data-ttu-id="f4842-167">Script de Python donde se especifican todos los parámetros</span><span class="sxs-lookup"><span data-stu-id="f4842-167">Python script specifying all parameters</span></span>
  <span data-ttu-id="f4842-168">readme.md</span><span class="sxs-lookup"><span data-stu-id="f4842-168">readme.md</span></span>|                           <span data-ttu-id="f4842-169">El presente documento Léame</span><span class="sxs-lookup"><span data-stu-id="f4842-169">This readme document</span></span>


## <a name="data-description"></a><span data-ttu-id="f4842-170">Descripción de los datos</span><span class="sxs-lookup"><span data-stu-id="f4842-170">Data description</span></span>

<span data-ttu-id="f4842-171">En este tutorial se usa como ejemplo un conjunto de datos con 428 imágenes de texturas de partes de arriba de ropa.</span><span class="sxs-lookup"><span data-stu-id="f4842-171">This tutorial uses as running example an upper body clothing texture dataset consisting of up to 428 images.</span></span> <span data-ttu-id="f4842-172">Cada imagen está anotada como perteneciente a una de tres posibles texturas distintas (lunares, rayas o leopardo).</span><span class="sxs-lookup"><span data-stu-id="f4842-172">Each image is annotated as one of three different textures (dotted, striped, leopard).</span></span> <span data-ttu-id="f4842-173">Hemos usado un número de imágenes reducido para que este tutorial sea rápido de ejecutar,</span><span class="sxs-lookup"><span data-stu-id="f4842-173">We kept the number of images small so that this tutorial can be executed quickly.</span></span> <span data-ttu-id="f4842-174">pero el código está totalmente probado y funciona con miles de imágenes o más.</span><span class="sxs-lookup"><span data-stu-id="f4842-174">However, the code is well-tested and works with tens of thousands of images or more.</span></span> <span data-ttu-id="f4842-175">Todas las imágenes se extrajeron con búsquedas de imágenes de Bing y se anotaron a mano tal y como se explica en la [parte 3](#using-a-custom-dataset).</span><span class="sxs-lookup"><span data-stu-id="f4842-175">All images were scraped using Bing Image Search and hand-annotated as is explained in [Part 3](#using-a-custom-dataset).</span></span> <span data-ttu-id="f4842-176">Las direcciones URL de las imágenes aparecen en el archivo */resources/fashionTextureUrls.tsv* junto con sus correspondientes atributos.</span><span class="sxs-lookup"><span data-stu-id="f4842-176">The image URLs with their respective attributes are listed in the */resources/fashionTextureUrls.tsv* file.</span></span>

<span data-ttu-id="f4842-177">El script `0_downloadData.py` descarga todas las imágenes en el directorio *DATA_DIR/images/fashionTexture/*.</span><span class="sxs-lookup"><span data-stu-id="f4842-177">The script `0_downloadData.py` downloads all images to the *DATA_DIR/images/fashionTexture/* directory.</span></span> <span data-ttu-id="f4842-178">Es probable que los vínculos de algunas de las 428 direcciones URL estén rotos.</span><span class="sxs-lookup"><span data-stu-id="f4842-178">Some of the 428 URLs are likely broken.</span></span> <span data-ttu-id="f4842-179">Esto no supone un problema y simplemente significa que dispondremos de menos imágenes con las que entrenar y probar.</span><span class="sxs-lookup"><span data-stu-id="f4842-179">This is not an issue, and just means that we have slightly fewer images for training and testing.</span></span> <span data-ttu-id="f4842-180">Todos los scripts que se proporcionan en este ejemplo tienen que ejecutarse de forma local y no en entornos como, por ejemplo, un entorno remoto de docker.</span><span class="sxs-lookup"><span data-stu-id="f4842-180">All scripts provided in this sample have to be executed locally, and not on e.g. a docker remote environment.</span></span>

<span data-ttu-id="f4842-181">En la siguiente ilustración se muestran ejemplos de los atributos de lunares (izquierda), a rayas (centro) y leopardo (derecha).</span><span class="sxs-lookup"><span data-stu-id="f4842-181">The following figure shows examples for the attributes dotted (left), striped (middle), and leopard (right).</span></span> <span data-ttu-id="f4842-182">Se realizaron las anotaciones pertinentes en función de la prenda de ropa.</span><span class="sxs-lookup"><span data-stu-id="f4842-182">Annotations were done according to the upper body clothing item.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/examples_all.jpg"  alt="alt text" width="700">
</p>


## <a name="part-1---model-training-and-evaluation"></a><span data-ttu-id="f4842-183">Parte 1: Entrenamiento y evaluación del modelo</span><span class="sxs-lookup"><span data-stu-id="f4842-183">Part 1 - Model training and evaluation</span></span>

<span data-ttu-id="f4842-184">En la primera parte de este tutorial, entrenaremos un sistema que usa (pero no modifica) una red neuronal profunda ya entrenada.</span><span class="sxs-lookup"><span data-stu-id="f4842-184">In the first part of this tutorial, we are training a system that uses, but does not modify, a pre-trained deep neural network.</span></span> <span data-ttu-id="f4842-185">Esta DNN ya entrenada se usa para la captura de características, y se entrena una SVM lineal para predecir el atributo (esto es, lunares, rayas o leopardo) de una imagen determinada.</span><span class="sxs-lookup"><span data-stu-id="f4842-185">This pre-trained DNN is used as a featurizer, and a linear SVM is trained to predict the attribute (dotted, striped, or leopard) of a given image.</span></span>

<span data-ttu-id="f4842-186">Ahora pasaremos a describir este método detalladamente y a indicar los scripts que hay que ejecutar.</span><span class="sxs-lookup"><span data-stu-id="f4842-186">We now described this approach in detail, step-by-step, and show which scripts need to be executed.</span></span> <span data-ttu-id="f4842-187">Es recomendable que, después de cada paso, se inspeccione qué archivos se han escrito y dónde se han escrito.</span><span class="sxs-lookup"><span data-stu-id="f4842-187">We recommend after each step to inspect which files are written and where they are written to.</span></span>

<span data-ttu-id="f4842-188">Todos los parámetros importantes (además de una breve explicación) se especifican en un único lugar: el archivo `PARAMETERS.py`.</span><span class="sxs-lookup"><span data-stu-id="f4842-188">All important parameters are specified, and a short explanation provided, in a single place: the  `PARAMETERS.py` file.</span></span>




### <a name="step-1-data-preparation"></a><span data-ttu-id="f4842-189">Paso 1: Preparación de los datos</span><span class="sxs-lookup"><span data-stu-id="f4842-189">Step 1: Data preparation</span></span>
`Script: 1_prepareData.py. Notebook: showImages.ipynb`

<span data-ttu-id="f4842-190">El bloc de notas `showImages.ipynb` se puede usar para ver las imágenes y para corregir su anotación según sea necesario.</span><span class="sxs-lookup"><span data-stu-id="f4842-190">The notebook `showImages.ipynb` can be used to visualize the images, and to correct their annotation as needed.</span></span> <span data-ttu-id="f4842-191">Para ejecutar el bloc de notas, ábralo en Azure Machine Learning Workbench y haga clic en "Start Notebook Server" (Iniciar el servidor del bloc de notas) si esta opción aparece; a continuación, cambie al kernel del proyecto local denominado "PROJECTNAME local" (por ejemplo,. "myImgClassUsingCNTK local") y ejecute todas las celdas del bloc de notas.</span><span class="sxs-lookup"><span data-stu-id="f4842-191">To run the notebook, open it in Azure Machine Learning Workbench, click on "Start Notebook Server" if this option is shown, change to the local project kernel with name "PROJECTNAME local" (e.g. "myImgClassUsingCNTK local"), and then execute all cells in the notebook.</span></span> <span data-ttu-id="f4842-192">Vea la sección Solución de problemas de este documento si se produce un error que indica que el bloc de notas es demasiado grande para mostrarse.</span><span class="sxs-lookup"><span data-stu-id="f4842-192">See the troubleshooting section in this document if you get an error complaining that the notebook is too large to be displayed.</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/notebook_showImages.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="f4842-193">Ahora, ejecute el script `1_prepareData.py`, que asigna todas las imágenes al conjunto de entrenamiento o al conjunto de prueba.</span><span class="sxs-lookup"><span data-stu-id="f4842-193">Now execute the script named `1_prepareData.py`, which assigns all images to either the training set or the test set.</span></span> <span data-ttu-id="f4842-194">Estas asignaciones son excluyentes entre sí; es decir, ninguna imagen de entrenamiento se usa para las pruebas y viceversa.</span><span class="sxs-lookup"><span data-stu-id="f4842-194">This assignment is mutually exclusive - no training image is also used for testing or vice versa.</span></span> <span data-ttu-id="f4842-195">Un 75 % de las imágenes de cada tipo se asigna de forma predeterminada y aleatoria al entrenamiento y el 25 por ciento restante, a las pruebas.</span><span class="sxs-lookup"><span data-stu-id="f4842-195">By default, a random 75% of the images from each attribute class are assigned to training, and the remaining 25% are assigned to testing.</span></span> <span data-ttu-id="f4842-196">Todos los datos generados por el script se guardan en la carpeta *DATA_DIR/proc/fashionTexture/*.</span><span class="sxs-lookup"><span data-stu-id="f4842-196">All data generated by the script are saved in the *DATA_DIR/proc/fashionTexture/* folder.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_1_white.jpg" alt="alt text" width="700"/>
</p>



### <a name="step-2-refining-the-deep-neural-network"></a><span data-ttu-id="f4842-197">Paso 2: Perfeccionar la red neuronal profunda</span><span class="sxs-lookup"><span data-stu-id="f4842-197">Step 2: Refining the Deep Neural Network</span></span>
`Script: 2_refineDNN.py`

<span data-ttu-id="f4842-198">Como ya hemos explicado en la parte 1 de este tutorial, la DNN ya entrenada se mantiene inamovible (es decir, no se perfecciona).</span><span class="sxs-lookup"><span data-stu-id="f4842-198">As we explained in part 1 of this tutorial, the pre-trained DNN is kept fixed (that is, it is not refined).</span></span> <span data-ttu-id="f4842-199">Pero el script `2_refineDNN.py` se ejecuta en la parte 1, dado que carga un modelo de [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) ya entrenado [2] y lo modifica (para, por ejemplo, permitir mayores resoluciones de imagen de entrada).</span><span class="sxs-lookup"><span data-stu-id="f4842-199">However, the script named `2_refineDNN.py` is still executed in part 1, as it loads a pre-trained [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) [2] model and modifies it, for example, to allow for higher input image resolution.</span></span> <span data-ttu-id="f4842-200">Este paso es rápido (tarda apenas segundos) y no requiere una GPU.</span><span class="sxs-lookup"><span data-stu-id="f4842-200">This step is fast (seconds) and does not require a GPU.</span></span>

<span data-ttu-id="f4842-201">En la parte 2 del tutorial, una modificación en el archivo PARAMETERS.py hace que el script `2_refineDNN.py` también perfeccione la DNN ya entrenada.</span><span class="sxs-lookup"><span data-stu-id="f4842-201">In part 2 of the tutorial, a modification to the PARAMETERS.py file causes the `2_refineDNN.py` script to also refine the pre-trained DNN.</span></span> <span data-ttu-id="f4842-202">Durante este perfeccionamiento, se ejecutan 45 tandas de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="f4842-202">By default, we run 45 training epochs during refinement.</span></span>

<span data-ttu-id="f4842-203">En uno y otro caso, el modelo final se escribe en el archivo *DATA_DIR/proc/fashionTexture/cntk_fixed.model*.</span><span class="sxs-lookup"><span data-stu-id="f4842-203">In both cases, the final model is then written to the file *DATA_DIR/proc/fashionTexture/cntk_fixed.model*.</span></span>

### <a name="step-3-evaluate-dnn-for-all-images"></a><span data-ttu-id="f4842-204">Paso 3: Evaluar la DNN para todas las imágenes</span><span class="sxs-lookup"><span data-stu-id="f4842-204">Step 3: Evaluate DNN for all images</span></span>
`Script: 3_runDNN.py`

<span data-ttu-id="f4842-205">Ahora, podemos usar la DNN (posiblemente mejorada) del último paso para caracterizar nuestras imágenes.</span><span class="sxs-lookup"><span data-stu-id="f4842-205">We can now use the (possibly refined) DNN from the last step to featurize our images.</span></span> <span data-ttu-id="f4842-206">Dada una imagen como entrada de la DNN, la salida es el vector de 512 flotantes de la penúltima capa del modelo.</span><span class="sxs-lookup"><span data-stu-id="f4842-206">Given an image as input to the DNN, the output is the 512-floats vector from the penultimate layer of the model.</span></span> <span data-ttu-id="f4842-207">Este vector tiene unas dimensiones mucho más pequeñas que la propia imagen.</span><span class="sxs-lookup"><span data-stu-id="f4842-207">This vector is much smaller dimensional than the image itself.</span></span> <span data-ttu-id="f4842-208">A pesar de ello, debe contener (y resaltar incluso) toda la información de la imagen que sea relevante para identificar el atributo de la imagen (esto es, si la prenda de ropa es de lunares, a rayas o con estampado de leopardo).</span><span class="sxs-lookup"><span data-stu-id="f4842-208">Nevertheless, it should contain (and even highlight) all information in the image relevant to recognize the image's attribute, that is, if the clothing item has a dotted, striped, or leopard texture.</span></span>

<span data-ttu-id="f4842-209">Todas las representaciones de imagen de la DNN se guardan en el archivo *DATA_DIR/proc/fashionTexture/cntkFiles/features.pickle*.</span><span class="sxs-lookup"><span data-stu-id="f4842-209">All of the DNN image representations are saved to the file *DATA_DIR/proc/fashionTexture/cntkFiles/features.pickle*.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_4_white.jpg" alt="alt text" width="700"/>
</p>


### <a name="step-4-support-vector-machine-training"></a><span data-ttu-id="f4842-210">Paso 4: Entrenamiento de una máquina de vectores de soporte</span><span class="sxs-lookup"><span data-stu-id="f4842-210">Step 4: Support Vector Machine training</span></span>
`Script: 4_trainSVM.py`

<span data-ttu-id="f4842-211">La representación de 512 flotantes calculada en el último paso se usa para entrenar un clasificador de SVM: dada una imagen como entrada, la SVM genera una puntuación para cada atributo que esté presente.</span><span class="sxs-lookup"><span data-stu-id="f4842-211">The 512-floats representations computed in the last step are now used to train an SVM classifier: given an image as input, the SVM outputs a score for each attribute to be present.</span></span> <span data-ttu-id="f4842-212">En nuestro conjunto de datos de ejemplo, esto significa una puntuación para "rayas", para "lunares" y para "leopardo".</span><span class="sxs-lookup"><span data-stu-id="f4842-212">In our example dataset, this means a score for 'striped', for 'dotted', and for 'leopard'.</span></span>

<span data-ttu-id="f4842-213">El script `4_trainSVM.py` carga las imágenes de entrenamiento, entrena una SVM para los distintos valores del parámetro de regularización (margen de demora) C y mantiene la SVM con la máxima precisión.</span><span class="sxs-lookup"><span data-stu-id="f4842-213">Script `4_trainSVM.py` loads the training images, trains an SVM for different values of the regularization (slack) parameter C, and keeps the SVM with highest accuracy.</span></span> <span data-ttu-id="f4842-214">La precisión de la clasificación se imprime en la consola y se traza en Workbench.</span><span class="sxs-lookup"><span data-stu-id="f4842-214">The classification accuracy is printed on the console and plotted in the Workbench.</span></span> <span data-ttu-id="f4842-215">En el caso de los datos de textura proporcionados, estos valores deben rondar el 100 % y el 88 % respectivamente.</span><span class="sxs-lookup"><span data-stu-id="f4842-215">For the provided texture data these values should be around 100% and 88% respectively.</span></span> <span data-ttu-id="f4842-216">Por último, la SVM entrenada se escribe en el archivo *DATA_DIR/proc/fashionTexture/cntkFiles/svm.np*.</span><span class="sxs-lookup"><span data-stu-id="f4842-216">Finally, the trained SVM is written to the file *DATA_DIR/proc/fashionTexture/cntkFiles/svm.np*.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/vienna_svm_log_zoom.jpg" alt="alt text" width="700"/>
</p>



### <a name="step-5-evaluation-and-visualization"></a><span data-ttu-id="f4842-217">Paso 5: Evaluación y visualización</span><span class="sxs-lookup"><span data-stu-id="f4842-217">Step 5: Evaluation and visualization</span></span>
`Script: 5_evaluate.py. Notebook: showResults.ipynb`

<span data-ttu-id="f4842-218">La precisión del clasificador de imágenes entrenado se puede medir con el script `5_evaluate.py`.</span><span class="sxs-lookup"><span data-stu-id="f4842-218">The accuracy of the trained image classifier can be measured using the script `5_evaluate.py`.</span></span> <span data-ttu-id="f4842-219">El script puntúa todas las imágenes de prueba con el clasificador de SVM entrenado, asigna a cada imagen el atributo con la puntuación más alta y compara los atributos de predicción con anotaciones precisas.</span><span class="sxs-lookup"><span data-stu-id="f4842-219">The script scores all test images using the trained SVM classifier, assigns each image the attribute with the highest score, and compares the predicted attributes with the ground truth annotations.</span></span>

<span data-ttu-id="f4842-220">Aquí mostramos la salida del script `5_evaluate.py`.</span><span class="sxs-lookup"><span data-stu-id="f4842-220">The output of script `5_evaluate.py` is shown below.</span></span> <span data-ttu-id="f4842-221">Se calcula la precisión de la clasificación de cada clase individual, así como la precisión del conjunto de prueba completo ("precisión total") y el promedio de las precisiones individuales ("promedio total de precisión de clases").</span><span class="sxs-lookup"><span data-stu-id="f4842-221">The classification accuracy of each individual class is computed, as well as the accuracy for the full test set ('overall accuracy'), and the average over the individual accuracies ('overall class-averaged accuracy').</span></span> <span data-ttu-id="f4842-222">100 % corresponde a la mejor precisión posible y 0 %, a la peor.</span><span class="sxs-lookup"><span data-stu-id="f4842-222">100% corresponds to the best possible accuracy and 0% to the worst.</span></span> <span data-ttu-id="f4842-223">Una estimación aleatoria generaría un promedio total de precisión de clases de 1 sobre el número de atributos: en nuestro caso, esta precisión sería pues del 33,33 %.</span><span class="sxs-lookup"><span data-stu-id="f4842-223">Random guessing would on average produce a class-averaged accuracy of 1 over the number of attributes: in our case, this accuracy would be 33.33%.</span></span> <span data-ttu-id="f4842-224">Estos resultados mejoran significativamente cuando se usa una mayor resolución de entrada, como `rf_inputResoluton = 1000`, si bien esto conlleva tiempos de cálculo de DNN más prolongados.</span><span class="sxs-lookup"><span data-stu-id="f4842-224">These results improve significantly when using a higher input resolution such as `rf_inputResoluton = 1000`, however at the expense of longer DNN computation times.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_6_white.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="f4842-225">Además de la precisión, la curva ROC se traza con la correspondiente área bajo la curva (izquierda). También mostramos la matriz de confusión (derecha):</span><span class="sxs-lookup"><span data-stu-id="f4842-225">In addition to accuracy, the ROC curve is plotted with respective area-under-curve (left); and the confusion matrix is shown (right):</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/roc_confMat.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="f4842-226">Por último, se proporciona el bloc de notas `showResults.py` para desplazarse por las imágenes de prueba y ver las correspondientes puntuaciones de clasificación.</span><span class="sxs-lookup"><span data-stu-id="f4842-226">Finally, the notebook `showResults.py` is provided to scroll through the test images and visualize their respective classification scores.</span></span> <span data-ttu-id="f4842-227">Tal como se explica en el paso 1, cada bloc de notas de este ejemplo debe usar el kernel del proyecto local denominado "PROJECTNAME local":</span><span class="sxs-lookup"><span data-stu-id="f4842-227">As explained in step1, every notebook in this sample needs to use the local project kernel with name "PROJECTNAME local":</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/notebook_showResults.jpg" alt="alt text" width="700"/>
</p>





### <a name="step-6-deployment"></a><span data-ttu-id="f4842-228">Paso 6: Implementación</span><span class="sxs-lookup"><span data-stu-id="f4842-228">Step 6: Deployment</span></span>
`Scripts: 6_callWebservice.py, deploymain.py. Notebook: deploy.ipynb`

<span data-ttu-id="f4842-229">El sistema entrenado ya se puede publicar como una API de REST.</span><span class="sxs-lookup"><span data-stu-id="f4842-229">The trained system can now be published as a REST API.</span></span> <span data-ttu-id="f4842-230">La implementación se explica en el bloc de notas `deploy.ipynb` y se basa en la funcionalidad de Azure Machine Learning Workbench (recuerde establecer como kernel el kernel del proyecto local denominado "PROJECTNAME local").</span><span class="sxs-lookup"><span data-stu-id="f4842-230">Deployment is explained in the notebook `deploy.ipynb`, and based on functionality within the Azure Machine Learning Workbench (remember to set as kernel the local project kernel with name "PROJECTNAME local").</span></span> <span data-ttu-id="f4842-231">Si quiere obtener más información detallada de la implementación, consulte también la excelente sección de implementación del [tutorial de IRIS](tutorial-classifying-iris-part-3.md).</span><span class="sxs-lookup"><span data-stu-id="f4842-231">See also the excellent deployment section of the [IRIS tutorial](tutorial-classifying-iris-part-3.md) for more deployment related information.</span></span>

<span data-ttu-id="f4842-232">Una vez implementado, el servicio web se puede llamar con el script `6_callWebservice.py`.</span><span class="sxs-lookup"><span data-stu-id="f4842-232">Once deployed, the web service can be called using the script `6_callWebservice.py`.</span></span> <span data-ttu-id="f4842-233">Cabe mencionar que la dirección IP (ya sea local o en la nube) del servicio web debe establecerse antes en el script.</span><span class="sxs-lookup"><span data-stu-id="f4842-233">Note that the IP address (either local or on the cloud) of the web service needs to be set first in the script.</span></span> <span data-ttu-id="f4842-234">En el bloc de notas `deploy.ipynb` se explica cómo encontrar esta dirección IP.</span><span class="sxs-lookup"><span data-stu-id="f4842-234">The notebook `deploy.ipynb` explains how to find this IP address.</span></span>








## <a name="part-2---accuracy-improvements"></a><span data-ttu-id="f4842-235">Parte 2: Mejoras en la precisión</span><span class="sxs-lookup"><span data-stu-id="f4842-235">Part 2 - Accuracy improvements</span></span>

<span data-ttu-id="f4842-236">En la parte 1 hemos visto cómo clasificar una imagen, entrenando para ello una máquina de vectores de soporte lineal en la salida de 512 flotantes de una red neuronal profunda.</span><span class="sxs-lookup"><span data-stu-id="f4842-236">In part 1, we showed how to classify an image by training a linear Support Vector Machine on the 512-floats output of a Deep Neural Network.</span></span> <span data-ttu-id="f4842-237">Esta DNN se entrenó previamente con millones de imágenes y la penúltima capa se devolvió como un vector de características.</span><span class="sxs-lookup"><span data-stu-id="f4842-237">This DNN was pre-trained on millions of images, and the penultimate layer returned as feature vector.</span></span> <span data-ttu-id="f4842-238">Este método es rápido, porque la DNN se usa tal cual y, con todo, da buenos resultados con frecuencia.</span><span class="sxs-lookup"><span data-stu-id="f4842-238">This approach is fast since the DNN is used as-is, but nevertheless often gives good results.</span></span>

<span data-ttu-id="f4842-239">Ahora nos centraremos en diversas maneras de mejorar la precisión del modelo de la parte 1.</span><span class="sxs-lookup"><span data-stu-id="f4842-239">We now present several ways to improve the accuracy of the model from part 1.</span></span> <span data-ttu-id="f4842-240">En concreto, vamos a perfeccionar la DNN en lugar mantenerla inamovible.</span><span class="sxs-lookup"><span data-stu-id="f4842-240">Most notably we refine the DNN rather than keeping it fixed.</span></span>

### <a name="dnn-refinement"></a><span data-ttu-id="f4842-241">Perfeccionamiento de la DNN</span><span class="sxs-lookup"><span data-stu-id="f4842-241">DNN refinement</span></span>

<span data-ttu-id="f4842-242">En lugar de en una SVM, la clasificación se puede realizar directamente en la red neuronal.</span><span class="sxs-lookup"><span data-stu-id="f4842-242">Instead of an SVM, one can do the classification directly in the neural network.</span></span> <span data-ttu-id="f4842-243">Esto se logra agregando una nueva última capa a la DNN ya entrenada, que toma los 512 flotantes de la penúltima capa como entrada.</span><span class="sxs-lookup"><span data-stu-id="f4842-243">This is achieved by adding a new last layer to the pre-trained DNN, which takes the 512-floats from the penultimate layer as input.</span></span> <span data-ttu-id="f4842-244">La ventaja de realizar la clasificación en la DNN es que ahora la red entera se puede volver a entrenar con una nueva propagación.</span><span class="sxs-lookup"><span data-stu-id="f4842-244">The advantage of doing the classification in the DNN is that now the full network can be retrained using backpropagation.</span></span> <span data-ttu-id="f4842-245">Con este método se suele obtener una precisión de la clasificación mucho mejor a si usáramos la DNN ya entrenada tal cual, pero conlleva unos tiempos de cálculo de DNN más prolongados (incluso con una GPU).</span><span class="sxs-lookup"><span data-stu-id="f4842-245">This approach often leads to much better classification accuracies compared to using the pre-trained DNN as-is, however at the expense of much longer training time (even with GPU).</span></span>

<span data-ttu-id="f4842-246">Entrenar la red neuronal en lugar de una SVM se efectúa cambiando la variable `classifier` en `PARAMETERS.py` de `svm` a `dnn`.</span><span class="sxs-lookup"><span data-stu-id="f4842-246">Training the Neural Network instead of an SVM is done by changing the variable `classifier` in `PARAMETERS.py` from `svm` to `dnn`.</span></span> <span data-ttu-id="f4842-247">Luego, tal y como se describe en la parte 1, hay que volver a ejecutar todos los scripts, excepto el de preparación de los datos (paso 1) y el de entrenamiento de SVM (paso 4).</span><span class="sxs-lookup"><span data-stu-id="f4842-247">Then, as described in part 1, all the scripts except for data preparation (step 1) and SVM training (step 4) need to be executed again.</span></span> <span data-ttu-id="f4842-248">El perfeccionamiento de la DNN requiere una GPU.</span><span class="sxs-lookup"><span data-stu-id="f4842-248">DNN refinement requires a GPU.</span></span> <span data-ttu-id="f4842-249">Si no existe ninguna GPU o si la GPU está bloqueada (por ejemplo, porque haya una ejecución de CNTK anterior), el script `2_refineDNN.py` generará un error.</span><span class="sxs-lookup"><span data-stu-id="f4842-249">if no GPU was found or if the GPU is locked (for example by a previous CNTK run) then script `2_refineDNN.py` throws an error.</span></span> <span data-ttu-id="f4842-250">En algunas GPU, el entrenamiento de la DNN puede producir un error de memoria insuficiente, que se puede soslayar si se reduce el tamaño del minilote (variable `cntk_mb_size` en `PARAMETERS.py`).</span><span class="sxs-lookup"><span data-stu-id="f4842-250">DNN training can throw out-of-memory error on some GPUs, which can be avoided by reducing the minibatch size (variable `cntk_mb_size` in `PARAMETERS.py`).</span></span>

<span data-ttu-id="f4842-251">Una vez completado el entrenamiento, el modelo perfeccionado se guarda en *DATA_DIR/proc/fashionTexture/cntk_refined.model* y se traza un gráfico que muestra cómo varían los errores de clasificación de entrenamiento y de prueba durante el entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="f4842-251">Once training completes, the refined model is saved to *DATA_DIR/proc/fashionTexture/cntk_refined.model*, and a plot drawn which shows how the training and test classification errors change during training.</span></span> <span data-ttu-id="f4842-252">Observe en ese gráfico que el error del conjunto de entrenamiento es mucho menor que el del conjunto de prueba.</span><span class="sxs-lookup"><span data-stu-id="f4842-252">Note in that plot that the error on the training set is much smaller than on the test set.</span></span> <span data-ttu-id="f4842-253">Este comportamiento, conocido como sobreajuste, se puede minimizar, por ejemplo, usando un valor más alto como tasa de eliminación (`rf_dropoutRate`).</span><span class="sxs-lookup"><span data-stu-id="f4842-253">This so-called over-fitting behavior can be reduced, for example,  by using a higher value for the dropout rate `rf_dropoutRate`.</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_3_plot.png" alt="alt text" height="300"/>
</p>

<span data-ttu-id="f4842-254">Como se aprecia en el siguiente gráfico, la precisión del conjunto de datos proporcionado usando el perfeccionamiento de la DNN es del 92,35 % en comparación con el 88,92 % previo (parte 1).</span><span class="sxs-lookup"><span data-stu-id="f4842-254">As can be seen in the plot below, the accuracy using DNN refinement on the provided dataset is 92.35% versus the 88.92% before (part 1).</span></span> <span data-ttu-id="f4842-255">En concreto, las imágenes relativas a los "lunares" (dotted) mejoran considerablemente y logran un área bajo la curva de ROC de 0,98 con el perfeccionamiento, frente al 0,94 anterior.</span><span class="sxs-lookup"><span data-stu-id="f4842-255">In particular, the 'dotted' images improve significantly, with an ROC area-under-curve of 0.98 with refinement vs. 0.94 before.</span></span> <span data-ttu-id="f4842-256">Estamos usando un conjunto de datos pequeño y, por lo tanto, las precisiones reales al ejecutar el código son diferentes.</span><span class="sxs-lookup"><span data-stu-id="f4842-256">We are using a small dataset, and hence the actual accuracies running the code are different.</span></span> <span data-ttu-id="f4842-257">Esta discrepancia obedece a efectos estocásticos como la división aleatoria de las imágenes en los conjuntos de pruebas de entrenamiento y de prueba.</span><span class="sxs-lookup"><span data-stu-id="f4842-257">This discrepancy is due to stochastic effects such as the random split of the images into training and testing sets.</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/roc_confMat_dnn.jpg" alt="alt text" width="700"/>
</p>

### <a name="run-history-tracking"></a><span data-ttu-id="f4842-258">Ejecutar un seguimiento del historial</span><span class="sxs-lookup"><span data-stu-id="f4842-258">Run history tracking</span></span>

<span data-ttu-id="f4842-259">Azure Machine Learning Workbench almacena el historial de cada ejecución en Azure para, así, permitir que se puedan comparar dos o más ejecuciones con incluso semanas de diferencia.</span><span class="sxs-lookup"><span data-stu-id="f4842-259">The Azure Machine Learning Workbench stores the history of each run on Azure to allow comparison of two or more runs that are even weeks apart.</span></span> <span data-ttu-id="f4842-260">Esto se explica con más detalle en el [tutorial de Iris](tutorial-classifying-iris-part-2.md).</span><span class="sxs-lookup"><span data-stu-id="f4842-260">This is explained in detail in the [Iris tutorial](tutorial-classifying-iris-part-2.md).</span></span> <span data-ttu-id="f4842-261">También se ilustra en las siguientes capturas de pantalla, donde comparamos dos ejecuciones del script `5_evaluate.py`, ya sea por medio del perfeccionamiento de la DNN, es decir, `classifier = "dnn"` (número de ejecución 148) o del entrenamiento de la SVM, es decir, `classifier = "svm"` (número de ejecución 150).</span><span class="sxs-lookup"><span data-stu-id="f4842-261">It is also illustrated in the following screenshots where we compare two runs of the script `5_evaluate.py`, using either DNN refinement that is, `classifier = "dnn"`(run number 148) or SVM training that is, `classifier = "svm"` (run number 150).</span></span>

<span data-ttu-id="f4842-262">En la primera captura de pantalla, el perfeccionamiento de la DNN lleva a unas mejores precisiones que con el entrenamiento de SVM en todas las clases.</span><span class="sxs-lookup"><span data-stu-id="f4842-262">In the first screenshot, the DNN refinement leads to better accuracies than SVM training for all classes.</span></span> <span data-ttu-id="f4842-263">La segunda captura de pantalla muestra todas las métricas de las que se hace un seguimiento, incluido el clasificador que se usó.</span><span class="sxs-lookup"><span data-stu-id="f4842-263">The second screenshot shows all metrics that are being tracked, including what the classifier was.</span></span> <span data-ttu-id="f4842-264">Este seguimiento se realiza en el script `5_evaluate.py`, llamando al registrador de Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="f4842-264">This tracking is done in the script `5_evaluate.py` by calling the Azure Machine Learning Workbench logger.</span></span> <span data-ttu-id="f4842-265">El script también guarda la curva de ROC y la matriz de confusión en la carpeta *outputs*.</span><span class="sxs-lookup"><span data-stu-id="f4842-265">In addition, the script also saves the ROC curve and confusion matrix to the *outputs* folder.</span></span> <span data-ttu-id="f4842-266">Esta carpeta *outputs* es especial, en el sentido de que también se realiza un seguimiento de su contenido por medio de la característica de historial de Workbench. Por lo tanto, se puede tener acceso a sus archivos siempre que se quiera, independientemente de si las copias locales se han sobrescrito.</span><span class="sxs-lookup"><span data-stu-id="f4842-266">This *outputs* folder is special in that its content is also tracked by the Workbench history feature and hence the output files can be accessed at any time, regardless of whether local copies have been overwritten.</span></span>

<span data-ttu-id="f4842-267"><p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison1.jpg" alt="alt text" width="700"/> </p>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison2b.jpg" alt="alt text" width="700"/>
</p></span><span class="sxs-lookup"><span data-stu-id="f4842-267"><p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison1.jpg" alt="alt text" width="700"/> </p>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison2b.jpg" alt="alt text" width="700"/>
</p></span></span>


### <a name="parameter-tuning"></a><span data-ttu-id="f4842-268">Ajuste de parámetros</span><span class="sxs-lookup"><span data-stu-id="f4842-268">Parameter tuning</span></span>
<span data-ttu-id="f4842-269">Como sucede en la mayoría de los proyectos de aprendizaje automático, para obtener buenos resultados en un conjunto de datos nuevo, es necesario ajustar los parámetros con mucho cuidado, así como sopesar detenidamente diversas decisiones de diseño.</span><span class="sxs-lookup"><span data-stu-id="f4842-269">As is true for most machine learning projects, getting good results for a new dataset requires careful parameter tuning as well as evaluating different design decisions.</span></span> <span data-ttu-id="f4842-270">Para hacer estas tareas más sencillas, todos los parámetros importantes (además de una breve explicación) se especifican en un único lugar: el archivo `PARAMETERS.py`.</span><span class="sxs-lookup"><span data-stu-id="f4842-270">To help with these tasks, all important parameters are specified, and a short explanation provided, in a single place: the `PARAMETERS.py` file.</span></span>

<span data-ttu-id="f4842-271">Estos son algunos aspectos que sin duda se traducirán en mejoras:</span><span class="sxs-lookup"><span data-stu-id="f4842-271">Some of the most promising avenues for improvements are:</span></span>

- <span data-ttu-id="f4842-272">Calidad de los datos: asegúrese de que la calidad de los conjuntos de entrenamiento y de prueba es alta.</span><span class="sxs-lookup"><span data-stu-id="f4842-272">Data quality: Ensure the training and test sets have high quality.</span></span> <span data-ttu-id="f4842-273">Es decir, procure que las imágenes estén correctamente anotadas, que ha quitado las imágenes ambiguas (por ejemplo, prendas de ropa que tienen tanto rayas como lunares) y que los atributos son excluyentes entre sí (dicho de otro modo, que los ha elegido de tal forma que cada imagen pertenece a exactamente un atributo).</span><span class="sxs-lookup"><span data-stu-id="f4842-273">That is, the images are annotated correctly, ambiguous images removed (for example clothing items with both stripes and dots), and the attributes are mutually exclusive (that is, chosen such that each image belongs to exactly one attribute).</span></span>
- <span data-ttu-id="f4842-274">Hay constancia de que, si el objeto de interés es pequeño en la imagen, los métodos de clasificación de imágenes no funcionan bien.</span><span class="sxs-lookup"><span data-stu-id="f4842-274">If the object-of-interest is small in the image then Image classification approaches are known not to work well.</span></span> <span data-ttu-id="f4842-275">En tales casos, considere el uso de un método de detección de objetos, tal y como se describe en este [tutorial](https://github.com/Azure/ObjectDetectionUsingCntk).</span><span class="sxs-lookup"><span data-stu-id="f4842-275">In such cases consider using an object detection approach as described in this [tutorial](https://github.com/Azure/ObjectDetectionUsingCntk).</span></span>
- <span data-ttu-id="f4842-276">Perfeccionamiento de la DNN: seguramente, el parámetro más importante para realizar esta tarea correctamente sea la velocidad de aprendizaje `rf_lrPerMb`.</span><span class="sxs-lookup"><span data-stu-id="f4842-276">DNN refinement: The arguably most important parameter to get right is the learning rate `rf_lrPerMb`.</span></span> <span data-ttu-id="f4842-277">Si la precisión del conjunto de entrenamiento (primera figura de la parte 2) no ronda el 0-5 %, probablemente se deba a un error en la velocidad de aprendizaje.</span><span class="sxs-lookup"><span data-stu-id="f4842-277">If the accuracy on the training set (first figure in part 2) is not close to 0-5%, most likely it is due to a wrong the learning rate.</span></span> <span data-ttu-id="f4842-278">Los demás parámetros que comienzan por `rf_` no son tan importantes.</span><span class="sxs-lookup"><span data-stu-id="f4842-278">The other parameters starting with `rf_` are less important.</span></span> <span data-ttu-id="f4842-279">Normalmente, el error de entrenamiento debería disminuir exponencialmente y aproximarse a 0 % después del entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="f4842-279">Typically, the training error should decrement exponentially and be close to 0% after training.</span></span>
- <span data-ttu-id="f4842-280">Resolución de entrada: la resolución de imagen predeterminada es 224 x 224 píxeles.</span><span class="sxs-lookup"><span data-stu-id="f4842-280">Input resolution: The default image resolution is 224x224 pixels.</span></span> <span data-ttu-id="f4842-281">Si usa una resolución de imagen mayor (parámetro: `rf_inputResoluton`), por ejemplo, 448 x 448 o 896 x 896 píxeles, con frecuencia obtendrá una mejora significativa, pero el perfeccionamiento de la DNN tardará más en completarse.</span><span class="sxs-lookup"><span data-stu-id="f4842-281">Using higher image resolution (parameter: `rf_inputResoluton`) of, for example, 448x448 or 896x896 pixels often significant improves accuracy but slows down DNN refinement.</span></span> <span data-ttu-id="f4842-282">**Usar una mayor resolución es inocuo y casi siempre aumenta la precisión**.</span><span class="sxs-lookup"><span data-stu-id="f4842-282">**Using higher image resolution is nearly free lunch and almost always boosts accuracy**.</span></span>
- <span data-ttu-id="f4842-283">Sobreajuste de la DNN: procure que no haya mucha diferencia entre la precisión del conjunto de entrenamiento y la del conjunto de prueba durante el perfeccionamiento de la DNN (primera figura de la parte 2).</span><span class="sxs-lookup"><span data-stu-id="f4842-283">DNN over-fitting: Avoid a large gap between the training and test accuracy during DNN refinement (first figure in part 2).</span></span> <span data-ttu-id="f4842-284">Esta diferencia se puede reducir usando tasas de eliminación (`rf_dropoutRate`) de 0,5 o más, así como aumentando el peso del regularizador `rf_l2RegWeight`.</span><span class="sxs-lookup"><span data-stu-id="f4842-284">This gap can be reduced using dropout rates `rf_dropoutRate` of 0.5 or more, and by increasing the regularizer weight `rf_l2RegWeight`.</span></span> <span data-ttu-id="f4842-285">Usar una tasa de eliminación elevada puede ser especialmente útil si la resolución de imagen de entrada de DNN es alta.</span><span class="sxs-lookup"><span data-stu-id="f4842-285">Using a high dropout rate can be especially helpful if the DNN input image resolution is high.</span></span>
- <span data-ttu-id="f4842-286">Pruebe a usar DNN un poco más profundas, cambiando para ello `rf_pretrainedModelFilename` de `ResNet_18.model` a `ResNet_34.model` o a `ResNet_50.model`.</span><span class="sxs-lookup"><span data-stu-id="f4842-286">Try using deeper DNNs by changing `rf_pretrainedModelFilename` from `ResNet_18.model` to either `ResNet_34.model` or `ResNet_50.model`.</span></span> <span data-ttu-id="f4842-287">El modelo ResNet-50 no solo es más profundo, sino que, además, su salida de la penúltima capa tiene un tamaño de 2048 flotantes (frente a los 512 flotantes de los modelos ResNet-18 y ResNet 34).</span><span class="sxs-lookup"><span data-stu-id="f4842-287">The Resnet-50 model is not only deeper, but its output of the penultimate layer is of size 2048 floats (vs. 512 floats of the ResNet-18 and ResNet-34 models).</span></span> <span data-ttu-id="f4842-288">Esta mayor dimensión puede resultar especialmente útil al entrenar un clasificador de SVM.</span><span class="sxs-lookup"><span data-stu-id="f4842-288">This increased dimension can be especially beneficial when training an SVM classifier.</span></span>

## <a name="part-3---custom-dataset"></a><span data-ttu-id="f4842-289">Parte 3: Conjunto de datos personalizado</span><span class="sxs-lookup"><span data-stu-id="f4842-289">Part 3 - Custom dataset</span></span>

<span data-ttu-id="f4842-290">En las partes 1 y 2, hemos entrenado y evaluado un modelo de clasificación de imágenes usando las imágenes de texturas de partes de arriba de ropa proporcionadas.</span><span class="sxs-lookup"><span data-stu-id="f4842-290">In part 1 and 2, we trained and evaluated an image classification model using the provided upper body clothing textures images.</span></span> <span data-ttu-id="f4842-291">Ahora, describiremos cómo usar un conjunto de datos personalizado proporcionado por el usuario, en lugar de esas imágenes.</span><span class="sxs-lookup"><span data-stu-id="f4842-291">We now show how to use a custom user-provided dataset instead.</span></span> <span data-ttu-id="f4842-292">O bien, si no hay un conjunto disponible, cómo generar y anotar uno por medio de la búsqueda de imágenes de Bing.</span><span class="sxs-lookup"><span data-stu-id="f4842-292">Or, if not available, how to generate and annotate such a dataset using Bing Image Search.</span></span>

### <a name="using-a-custom-dataset"></a><span data-ttu-id="f4842-293">Usar un conjunto de datos personalizado</span><span class="sxs-lookup"><span data-stu-id="f4842-293">Using a custom dataset</span></span>

<span data-ttu-id="f4842-294">En primer lugar, echemos un vistazo a la estructura de carpetas de los datos de texturas de prendas de ropa.</span><span class="sxs-lookup"><span data-stu-id="f4842-294">First, let's have a look at the folder structure for the clothing texture data.</span></span> <span data-ttu-id="f4842-295">Observe cómo todas las imágenes relativas a cada atributo se encuentran en las subcarpetas correspondientes (*dotted*, \*leopard y *striped*) dentro de *DATA_DIR/images/fashionTexture/*.</span><span class="sxs-lookup"><span data-stu-id="f4842-295">Note how all images for the different attributes are in the respective subfolders *dotted*, \*leopard, and *striped* at *DATA_DIR/images/fashionTexture/*.</span></span> <span data-ttu-id="f4842-296">Observe también cómo el nombre de la carpeta de imágenes también aparece reflejado en el archivo `PARAMETERS.py`:</span><span class="sxs-lookup"><span data-stu-id="f4842-296">Note also how the image folder name also occurs in the `PARAMETERS.py` file:</span></span>
```python
datasetName = "fashionTexture"
```

<span data-ttu-id="f4842-297">Usar un conjunto de datos personalizado es tan sencillo como reproducir esta misma estructura de carpetas donde están todas las imágenes en subcarpetas según sus atributos y, luego, copiar estas subcarpetas en un nuevo directorio especificado por el usuario, *DATA_DIR/images/newDataSetName/*.</span><span class="sxs-lookup"><span data-stu-id="f4842-297">Using a custom dataset is as simple as reproducing this folder structure where all images are in subfolders according to their attribute, and to copy these subfolders to a new user-specified directory *DATA_DIR/images/newDataSetName/*.</span></span> <span data-ttu-id="f4842-298">El único cambio de código necesario consiste en establecer la variable `datasetName` en *newDataSetName*.</span><span class="sxs-lookup"><span data-stu-id="f4842-298">The only code change required is to set the `datasetName` variable to *newDataSetName*.</span></span> <span data-ttu-id="f4842-299">Tras ello, los scripts 1-5 se pueden ejecutar en orden y todos los archivos intermedios se escriben en *DATA_DIR/proc/newDataSetName/*.</span><span class="sxs-lookup"><span data-stu-id="f4842-299">Scripts 1-5 can then be executed in order, and all intermediate files are written to *DATA_DIR/proc/newDataSetName/*.</span></span> <span data-ttu-id="f4842-300">No se requiere ningún cambio de código más.</span><span class="sxs-lookup"><span data-stu-id="f4842-300">No other code changes are required.</span></span>

<span data-ttu-id="f4842-301">Es importante que cada imagen pueda asignarse a exactamente un solo atributo.</span><span class="sxs-lookup"><span data-stu-id="f4842-301">It is important that each image can be assigned to exactly one attribute.</span></span> <span data-ttu-id="f4842-302">Por ejemplo, sería incorrecto usar atributos para "animal" y para "leopardo", dado que "leopardo" también pertenecería a la clasificación "animal".</span><span class="sxs-lookup"><span data-stu-id="f4842-302">For example, it would be wrong to have attributes for 'animal' and for 'leopard', since a 'leopard' image would also belong to 'animal'.</span></span> <span data-ttu-id="f4842-303">De igual modo, conviene eliminar las imágenes ambiguas (y, por tanto, difíciles de anotar).</span><span class="sxs-lookup"><span data-stu-id="f4842-303">Also, it is best to remove images that are ambiguous and hence difficult to annotate.</span></span>



### <a name="image-scraping-and-annotation"></a><span data-ttu-id="f4842-304">Extracción y anotación de imágenes</span><span class="sxs-lookup"><span data-stu-id="f4842-304">Image scraping and annotation</span></span>

<span data-ttu-id="f4842-305">Recopilar un número suficientemente grande de imágenes anotadas para el entrenamiento y las pruebas puede resultar complicado.</span><span class="sxs-lookup"><span data-stu-id="f4842-305">Collecting a sufficiently large number of annotated images for training and testing can be difficult.</span></span> <span data-ttu-id="f4842-306">Una forma de solucionar este problema consiste en extraer imágenes de Internet.</span><span class="sxs-lookup"><span data-stu-id="f4842-306">One way to overcome this problem is to scrape images from the Internet.</span></span> <span data-ttu-id="f4842-307">Por ejemplo, eche un vistazo a los siguientes resultados de una búsqueda de imágenes de Bing a raíz de la consulta *t-shirt striped* para encontrar camisetas a rayas.</span><span class="sxs-lookup"><span data-stu-id="f4842-307">For example, see below the Bing Image Search results for the query *t-shirt striped*.</span></span> <span data-ttu-id="f4842-308">Como se esperaba, la mayoría de las imágenes son verdaderamente camisetas a rayas.</span><span class="sxs-lookup"><span data-stu-id="f4842-308">As expected, most images indeed are striped t-shirts.</span></span> <span data-ttu-id="f4842-309">Las pocas imágenes incorrectas o ambiguas (por ejemplo, la de la columna 1, fila 1, o la de la columna 3, fila 2) se pueden identificar y quitar fácilmente:</span><span class="sxs-lookup"><span data-stu-id="f4842-309">The few incorrect or ambiguous images (such as column 1, row 1; or column 3, row 2) can be identified and removed easily:</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/bing_search_striped.jpg" alt="alt text" width="600"/>
</p>

<span data-ttu-id="f4842-310">Para generar un conjunto de datos voluminoso y diverso, hay que realizar varias consultas.</span><span class="sxs-lookup"><span data-stu-id="f4842-310">To generate a large and diverse dataset, multiple queries should be used.</span></span> <span data-ttu-id="f4842-311">Por ejemplo, si tenemos 7 \* 3 = 21 consultas, se pueden sintetizar automáticamente usando todas las combinaciones de prendas de ropa {blusa, sudadera, sudadera con capucha, jersey, camisa, camiseta, chaleco} y atributos {rayas, lunares, leopardo}.</span><span class="sxs-lookup"><span data-stu-id="f4842-311">For example, 7\*3 = 21 queries can be synthesized automatically using all combinations of clothing items {blouse, hoodie, pullover, sweater, shirt, t-shirt, vest} and attributes {striped, dotted, leopard}.</span></span> <span data-ttu-id="f4842-312">Descargar las primeras 50 imágenes de cada consulta nos llevaría a tener un máximo de 21 x 50 = 1050 imágenes.</span><span class="sxs-lookup"><span data-stu-id="f4842-312">Downloading the top 50 images per query would then lead to a maximum of 21\*50=1050 images.</span></span>

<span data-ttu-id="f4842-313">En lugar de descargarlas manualmente desde la búsqueda de imágenes de Bing, es mucho más fácil usar en su lugar la [Bing Image Search API de Cognitive Services](https://www.microsoft.com/cognitive-services/bing-image-search-api), que devuelve un conjunto de direcciones URL de imágenes correspondientes a una cadena de consulta determinada.</span><span class="sxs-lookup"><span data-stu-id="f4842-313">Rather than manually downloading images from Bing Image Search, it is much easier to instead use the [Cognitive Services Bing Image Search API](https://www.microsoft.com/cognitive-services/bing-image-search-api) which returns a set of image URLs given a query string.</span></span>

<span data-ttu-id="f4842-314">Algunas de las imágenes descargadas son duplicados exactos o casi exactos (por ejemplo, solo se diferencian por la resolución de imagen o por anomalías del formato jpg).</span><span class="sxs-lookup"><span data-stu-id="f4842-314">Some of the downloaded images are exact or near duplicates (for example, differ just by image resolution or jpg artifacts).</span></span> <span data-ttu-id="f4842-315">Estos duplicados se deben quitar para que los conjuntos de entrenamiento y de prueba no contengan las mismas imágenes.</span><span class="sxs-lookup"><span data-stu-id="f4842-315">These duplicates should be removed so that the training and test split do not contain the same images.</span></span> <span data-ttu-id="f4842-316">La eliminación de imágenes duplicadas puede lograrse con un método basado en hash, consistente en dos pasos: (1) en primer lugar, se calcula la cadena hash de todas las imágenes y (2) en un segundo procesamiento de las imágenes, solamente se conservan aquellas que tengan una cadena hash que aún no se haya visto.</span><span class="sxs-lookup"><span data-stu-id="f4842-316">Removing duplicate images can be achieved using a hashing-based approach, which works in two steps: (i) first, the hash string is computed for all images; (ii) in a second pass over the images, only those images are kept with a hash string that has not yet been seen.</span></span> <span data-ttu-id="f4842-317">Todas las demás imágenes se descartan.</span><span class="sxs-lookup"><span data-stu-id="f4842-317">All other images are discarded.</span></span> <span data-ttu-id="f4842-318">Hemos constatado que el método `dhash` de la biblioteca de Python `imagehash` (descrito en este [blog](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html)) funciona bien con el parámetro `hash_size` establecido en 16.</span><span class="sxs-lookup"><span data-stu-id="f4842-318">We found the `dhash` approach in the Python library `imagehash` and described in this [blog](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html) to perform well, with the parameter `hash_size` set to 16.</span></span> <span data-ttu-id="f4842-319">No pasa nada si se quitan algunas imágenes no duplicadas por error, siempre y cuando se quiten la mayoría de los duplicados reales.</span><span class="sxs-lookup"><span data-stu-id="f4842-319">It is OK to incorrectly remove some non-duplicate images, as long as the majority of the real duplicates get removed.</span></span>





## <a name="conclusion"></a><span data-ttu-id="f4842-320">Conclusión</span><span class="sxs-lookup"><span data-stu-id="f4842-320">Conclusion</span></span>

<span data-ttu-id="f4842-321">Algunos aspectos destacados de este ejemplo son:</span><span class="sxs-lookup"><span data-stu-id="f4842-321">Some key highlights of this example are:</span></span>
- <span data-ttu-id="f4842-322">El código usado para entrenar, evaluar e implementar un modelo personalizado de clasificación de imágenes.</span><span class="sxs-lookup"><span data-stu-id="f4842-322">Code to train, evaluate, and deploy image classification models.</span></span>
- <span data-ttu-id="f4842-323">El uso de las imágenes de demostración suministradas, aunque esto es fácilmente adaptable (solo habría que cambiar una línea) para usar un conjunto de datos de imágenes propio.</span><span class="sxs-lookup"><span data-stu-id="f4842-323">Demo images provided, but easily adaptable (one line change) to use own image dataset.</span></span>
- <span data-ttu-id="f4842-324">Las características modernas y de nivel de experto implementadas para entrenar modelos de alta precisión basados en el aprendizaje de transferencia.</span><span class="sxs-lookup"><span data-stu-id="f4842-324">State-of-the-art expert features implemented to train high accuracy models based on Transfer Learning.</span></span>
- <span data-ttu-id="f4842-325">El desarrollo de modelos interactivos con Azure Machine Learning Workbench y Jupyter Notebook.</span><span class="sxs-lookup"><span data-stu-id="f4842-325">Interactive model development with Azure Machine Learning Workbench and Jupyter Notebook.</span></span>


## <a name="references"></a><span data-ttu-id="f4842-326">Referencias</span><span class="sxs-lookup"><span data-stu-id="f4842-326">References</span></span>

<span data-ttu-id="f4842-327">[1] Alex Krizhevsky, Ilya Sutskever y Geoffrey E. Hinton. [_ImageNet Classification with Deep Convolutional Neural Networks_](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (Clasificación de ImageNet con redes neuronales profundas de convolución).</span><span class="sxs-lookup"><span data-stu-id="f4842-327">[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, [_ImageNet Classification with Deep Convolutional Neural Networks_](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).</span></span> <span data-ttu-id="f4842-328">NIPS 2012.</span><span class="sxs-lookup"><span data-stu-id="f4842-328">NIPS 2012.</span></span>  
<span data-ttu-id="f4842-329">[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren y Jian Sun. [_Deep Residual Learning for Image Recognition_](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) (Aprendizaje residual profundo para el reconocimiento de imágenes).</span><span class="sxs-lookup"><span data-stu-id="f4842-329">[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, [_Deep Residual Learning for Image Recognition_](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf).</span></span> <span data-ttu-id="f4842-330">CVPR 2016.</span><span class="sxs-lookup"><span data-stu-id="f4842-330">CVPR 2016.</span></span>
