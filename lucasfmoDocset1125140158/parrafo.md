---
title: Clasificación de imágenes con CNTK en Azure Machine Learning Workbench | Microsoft Docs
description: 'Use Azure ML Workbench para probar, evaluar e implementar un modelo personalizado de clasificación de imágenes.'
services: machine-learning
documentationcenter: ''
author: PatrickBue
ms.author: pabuehle
manager: mwinkle
ms.reviewer: 'marhamil, mldocs, garyericson, jasonwhowell'
ms.service: machine-learning
ms.component: desktop-workbench
ms.workload: data-services
ms.topic: article
ms.date: 10/17/2017
ms.openlocfilehash: 5ff6502b0ed023f6fe8a9475a0e81991a9918cc5
ms.sourcegitcommit: 3c3488fb16a3c3287c3e1cd11435174711e92126
ms.translationtype: HT
ms.contentlocale: es-ES
ms.lasthandoff: 06/07/2018
ms.locfileid: '34850178'
---

# <a name="image-classification-using-azure-machine-learning-workbench"></a><span data-ttu-id="6aa19-103">Clasificación de imágenes con Azure Machine Learning Workbench</span><span class="sxs-lookup"><span data-stu-id="6aa19-103">Clasificación de imágenes con Azure Machine Learning Workbench</span></span>

<span data-ttu-id="6aa19-104">Se pueden usar distintos métodos de clasificación de imágenes para resolver un gran número de problemas de Computer Vision.</span><span class="sxs-lookup"><span data-stu-id="6aa19-104">Se pueden usar distintos métodos de clasificación de imágenes para resolver un gran número de problemas de Computer Vision.</span></span>
<span data-ttu-id="6aa19-105">Esto incluye la creación de modelos que respondan preguntas del tipo *¿Hay un OBJETO en la imagen?*, donde OBJETO puede ser, por ejemplo, un *perro*, un *coche* o un *barco*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-105">Esto incluye la creación de modelos que respondan preguntas del tipo *¿Hay un OBJETO en la imagen?*, donde OBJETO puede ser, por ejemplo, un *perro*, un *coche* o un *barco*.</span></span> <span data-ttu-id="6aa19-106">O a preguntas más complejas, como *¿Qué gravedad de enfermedad ocular revela el escáner de retina de este paciente?*</span><span class="sxs-lookup"><span data-stu-id="6aa19-106">O a preguntas más complejas, como *¿Qué gravedad de enfermedad ocular revela el escáner de retina de este paciente?*</span></span>

<span data-ttu-id="6aa19-107">En este tutorial se abordan estos problemas.</span><span class="sxs-lookup"><span data-stu-id="6aa19-107">En este tutorial se abordan estos problemas.</span></span> <span data-ttu-id="6aa19-108">En él explicamos cómo entrenar, evaluar e implementar su propio modelo de clasificación de imágenes usando [Microsoft Cognitive Toolkit (CNTK)](https://docs.microsoft.com/cognitive-toolkit/) para el aprendizaje profundo.</span><span class="sxs-lookup"><span data-stu-id="6aa19-108">En él explicamos cómo entrenar, evaluar e implementar su propio modelo de clasificación de imágenes usando [Microsoft Cognitive Toolkit (CNTK)](https://docs.microsoft.com/cognitive-toolkit/) para el aprendizaje profundo.</span></span>
<span data-ttu-id="6aa19-109">Se incluyen imágenes de ejemplo, si bien el lector puede aportar su propio conjunto de datos y entrenar modelos personalizados de su cosecha.</span><span class="sxs-lookup"><span data-stu-id="6aa19-109">Se incluyen imágenes de ejemplo, si bien el lector puede aportar su propio conjunto de datos y entrenar modelos personalizados de su cosecha.</span></span>

<span data-ttu-id="6aa19-110">Las soluciones de Computer Vision solían requerir un conocimiento de experto para identificar e implementar manualmente las llamadas *características*, que resaltan la información deseada en imágenes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-110">Las soluciones de Computer Vision solían requerir un conocimiento de experto para identificar e implementar manualmente las llamadas *características*, que resaltan la información deseada en imágenes.</span></span>
<span data-ttu-id="6aa19-111">Este método manual cambió en 2012 con el famoso artículo de [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) [1] sobre aprendizaje profundo y, en la actualidad, se usan redes neuronales profundas (DNN) para encontrar estas características automáticamente.</span><span class="sxs-lookup"><span data-stu-id="6aa19-111">Este método manual cambió en 2012 con el famoso artículo de [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) [1] sobre aprendizaje profundo y, en la actualidad, se usan redes neuronales profundas (DNN) para encontrar estas características automáticamente.</span></span>
<span data-ttu-id="6aa19-112">Las DNN supusieron una notable mejora en el campo no ya de la clasificación de imágenes, sino también para acabar con diversos problemas de Computer Vision como la detección de objetos y la similitud de imágenes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-112">Las DNN supusieron una notable mejora en el campo no ya de la clasificación de imágenes, sino también para acabar con diversos problemas de Computer Vision como la detección de objetos y la similitud de imágenes.</span></span>


## <a name="link-to-the-gallery-github-repository"></a><span data-ttu-id="6aa19-113">Vínculo al repositorio de GitHub de la galería</span><span class="sxs-lookup"><span data-stu-id="6aa19-113">Vínculo al repositorio de GitHub de la galería</span></span>
[https://github.com/Azure/MachineLearningSamples-ImageClassificationUsingCNTK](https://github.com/Azure/MachineLearningSamples-ImageClassificationUsingCNTK)

## <a name="overview"></a><span data-ttu-id="6aa19-114">Información general</span><span class="sxs-lookup"><span data-stu-id="6aa19-114">Información general</span></span>

<span data-ttu-id="6aa19-115">Este tutorial se divide en tres partes:</span><span class="sxs-lookup"><span data-stu-id="6aa19-115">Este tutorial se divide en tres partes:</span></span>

- <span data-ttu-id="6aa19-116">En la parte 1 se muestra cómo entrenar, evaluar e implementar un sistema de clasificación de imágenes, usando para ello una DNN ya entrenada para la captura de características y probando un SVM en la salida.</span><span class="sxs-lookup"><span data-stu-id="6aa19-116">En la parte 1 se muestra cómo entrenar, evaluar e implementar un sistema de clasificación de imágenes, usando para ello una DNN ya entrenada para la captura de características y probando un SVM en la salida.</span></span>
- <span data-ttu-id="6aa19-117">En la parte 2 siguiente se indica cómo mejorar la precisión, por ejemplo, perfeccionando la DNN en lugar de usarla para la captura de características.</span><span class="sxs-lookup"><span data-stu-id="6aa19-117">En la parte 2 siguiente se indica cómo mejorar la precisión, por ejemplo, perfeccionando la DNN en lugar de usarla para la captura de características.</span></span>
- <span data-ttu-id="6aa19-118">En la parte 3 se describe cómo puede usar un conjunto de datos de su cosecha en lugar de las imágenes de ejemplo suministradas y, si es necesario, cómo generar su propio conjunto de datos extrayendo imágenes de Internet.</span><span class="sxs-lookup"><span data-stu-id="6aa19-118">En la parte 3 se describe cómo puede usar un conjunto de datos de su cosecha en lugar de las imágenes de ejemplo suministradas y, si es necesario, cómo generar su propio conjunto de datos extrayendo imágenes de Internet.</span></span>

<span data-ttu-id="6aa19-119">No se requiere experiencia previa de aprendizaje automático y CNTK, pero sí resulta útil para comprender los principios subyacentes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-119">No se requiere experiencia previa de aprendizaje automático y CNTK, pero sí resulta útil para comprender los principios subyacentes.</span></span> <span data-ttu-id="6aa19-120">La precisión de los números, la duración del entrenamiento y otras cuestiones reflejadas en el tutorial se incluyen exclusivamente a modo de referencia; de igual modo, los valores reales que se obtengan al ejecutar el código variarán casi con toda seguridad.</span><span class="sxs-lookup"><span data-stu-id="6aa19-120">La precisión de los números, la duración del entrenamiento y otras cuestiones reflejadas en el tutorial se incluyen exclusivamente a modo de referencia; de igual modo, los valores reales que se obtengan al ejecutar el código variarán casi con toda seguridad.</span></span>


## <a name="prerequisites"></a><span data-ttu-id="6aa19-121">requisitos previos</span><span class="sxs-lookup"><span data-stu-id="6aa19-121">requisitos previos</span></span>

<span data-ttu-id="6aa19-122">Los requisitos previos para ejecutar este ejemplo son los siguientes:</span><span class="sxs-lookup"><span data-stu-id="6aa19-122">Los requisitos previos para ejecutar este ejemplo son los siguientes:</span></span>

1. <span data-ttu-id="6aa19-123">Una [cuenta de Azure](https://azure.microsoft.com/free/) (hay disponibles versiones gratuitas de prueba).</span><span class="sxs-lookup"><span data-stu-id="6aa19-123">Una [cuenta de Azure](https://azure.microsoft.com/free/) (hay disponibles versiones gratuitas de prueba).</span></span>
2. <span data-ttu-id="6aa19-124">Una instalación de [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md) siguiendo la [Guía de instalación de inicio rápido](../service/quickstart-installation.md) para instalar el programa y crear un área de trabajo.</span><span class="sxs-lookup"><span data-stu-id="6aa19-124">Una instalación de [Azure Machine Learning Workbench](../service/overview-what-is-azure-ml.md) siguiendo la [Guía de instalación de inicio rápido](../service/quickstart-installation.md) para instalar el programa y crear un área de trabajo.</span></span>  
3. <span data-ttu-id="6aa19-125">Un equipo Windows.</span><span class="sxs-lookup"><span data-stu-id="6aa19-125">Un equipo Windows.</span></span> <span data-ttu-id="6aa19-126">Se necesita un sistema operativo Windows porque Workbench admite únicamente Windows y Mac OS, mientras que Microsoft Cognitive Toolkit (que usaremos como biblioteca de aprendizaje profundo) solo es compatible con Windows y Linux.</span><span class="sxs-lookup"><span data-stu-id="6aa19-126">Se necesita un sistema operativo Windows porque Workbench admite únicamente Windows y Mac OS, mientras que Microsoft Cognitive Toolkit (que usaremos como biblioteca de aprendizaje profundo) solo es compatible con Windows y Linux.</span></span>
4. <span data-ttu-id="6aa19-127">No se necesita una GPU dedicada para ejecutar el entrenamiento de SVM en la parte 1, pero sí para el proceso de perfeccionamiento de la DNN descrito en la parte 2.</span><span class="sxs-lookup"><span data-stu-id="6aa19-127">No se necesita una GPU dedicada para ejecutar el entrenamiento de SVM en la parte 1, pero sí para el proceso de perfeccionamiento de la DNN descrito en la parte 2.</span></span> <span data-ttu-id="6aa19-128">Si no tiene una GPU en condiciones, quiere entrenar en varias GPU o no tiene un equipo con Windows, considere la posibilidad de usar máquina de virtual de aprendizaje profundo de Azure con un sistema operativo Windows.</span><span class="sxs-lookup"><span data-stu-id="6aa19-128">Si no tiene una GPU en condiciones, quiere entrenar en varias GPU o no tiene un equipo con Windows, considere la posibilidad de usar máquina de virtual de aprendizaje profundo de Azure con un sistema operativo Windows.</span></span> <span data-ttu-id="6aa19-129">Vaya [aquí](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-ads.dsvm-deep-learning) para obtener una guía de implementación con un solo clic.</span><span class="sxs-lookup"><span data-stu-id="6aa19-129">Vaya [aquí](https://azuremarketplace.microsoft.com/marketplace/apps/microsoft-ads.dsvm-deep-learning) para obtener una guía de implementación con un solo clic.</span></span> <span data-ttu-id="6aa19-130">Una vez implementada, conéctese a la máquina virtual a través de una conexión de Escritorio remoto, instale Workbench y ejecute el código localmente en ella.</span><span class="sxs-lookup"><span data-stu-id="6aa19-130">Una vez implementada, conéctese a la máquina virtual a través de una conexión de Escritorio remoto, instale Workbench y ejecute el código localmente en ella.</span></span>
5. <span data-ttu-id="6aa19-131">Es necesario instalar varias bibliotecas de Python, como OpenCV.</span><span class="sxs-lookup"><span data-stu-id="6aa19-131">Es necesario instalar varias bibliotecas de Python, como OpenCV.</span></span> <span data-ttu-id="6aa19-132">Haga clic en *Abrir símbolo del sistema* en el menú *Archivo* de Workbench y ejecute los siguientes comandos para instalar estas dependencias:</span><span class="sxs-lookup"><span data-stu-id="6aa19-132">Haga clic en *Abrir símbolo del sistema* en el menú *Archivo* de Workbench y ejecute los siguientes comandos para instalar estas dependencias:</span></span>  
    - `pip install https://cntk.ai/PythonWheel/GPU/cntk-2.2-cp35-cp35m-win_amd64.whl`  
    - <span data-ttu-id="6aa19-133">`pip install opencv_python-3.3.1-cp35-cp35m-win_amd64.whl` después de descargar OpenCV de http://www.lfd.uci.edu/~gohlke/pythonlibs/ (la versión y el nombre de archivo exactos pueden variar).</span><span class="sxs-lookup"><span data-stu-id="6aa19-133">`pip install opencv_python-3.3.1-cp35-cp35m-win_amd64.whl` después de descargar OpenCV de http://www.lfd.uci.edu/~gohlke/pythonlibs/ (la versión y el nombre de archivo exactos pueden variar).</span></span>
    - `conda install pillow`
    - `pip install -U numpy`
    - `pip install bqplot`
    - `jupyter nbextension enable --py --sys-prefix bqplot`
    - `jupyter nbextension enable --py widgetsnbextension`

### <a name="troubleshooting--known-bugs"></a><span data-ttu-id="6aa19-134">Solución de problemas/Errores conocidos</span><span class="sxs-lookup"><span data-stu-id="6aa19-134">Solución de problemas/Errores conocidos</span></span>
- <span data-ttu-id="6aa19-135">Se necesita una GPU para la parte 2.</span><span class="sxs-lookup"><span data-stu-id="6aa19-135">Se necesita una GPU para la parte 2.</span></span> <span data-ttu-id="6aa19-136">Si no hay una, cuando trate de perfeccionar la DNN aparecerá un error que indica que el aprendizaje de normalización por lotes todavía no se ha implementado.</span><span class="sxs-lookup"><span data-stu-id="6aa19-136">Si no hay una, cuando trate de perfeccionar la DNN aparecerá un error que indica que el aprendizaje de normalización por lotes todavía no se ha implementado.</span></span>
- <span data-ttu-id="6aa19-137">Los errores de memoria insuficiente durante el entrenamiento de DNN se pueden soslayar si se reduce el tamaño del minilote (variable `cntk_mb_size` en `PARAMETERS.py`).</span><span class="sxs-lookup"><span data-stu-id="6aa19-137">Los errores de memoria insuficiente durante el entrenamiento de DNN se pueden soslayar si se reduce el tamaño del minilote (variable `cntk_mb_size` en `PARAMETERS.py`).</span></span>
- <span data-ttu-id="6aa19-138">El código se probó con CNTK 2.2 y debería poder ejecutarse tanto en versiones anteriores (hasta 2.0) como en versiones más recientes sin cambios (o solo con cambios mínimos).</span><span class="sxs-lookup"><span data-stu-id="6aa19-138">El código se probó con CNTK 2.2 y debería poder ejecutarse tanto en versiones anteriores (hasta 2.0) como en versiones más recientes sin cambios (o solo con cambios mínimos).</span></span>
- <span data-ttu-id="6aa19-139">En el momento de redactar este artículo, Azure Machine Learning Workbench generaba problemas a la hora de mostrar blocs de notas superiores a 5 MB.</span><span class="sxs-lookup"><span data-stu-id="6aa19-139">En el momento de redactar este artículo, Azure Machine Learning Workbench generaba problemas a la hora de mostrar blocs de notas superiores a 5 MB.</span></span> <span data-ttu-id="6aa19-140">Puede haber blocs de notas con este tamaño si se guardan mostrando todas las salidas de celda.</span><span class="sxs-lookup"><span data-stu-id="6aa19-140">Puede haber blocs de notas con este tamaño si se guardan mostrando todas las salidas de celda.</span></span> <span data-ttu-id="6aa19-141">Si este error se produce, abra un símbolo del sistema en el menú Archivo de Workbench, ejecute `jupyter notebook`, abra el bloc de notas, borre todas las salidas y guárdelo.</span><span class="sxs-lookup"><span data-stu-id="6aa19-141">Si este error se produce, abra un símbolo del sistema en el menú Archivo de Workbench, ejecute `jupyter notebook`, abra el bloc de notas, borre todas las salidas y guárdelo.</span></span> <span data-ttu-id="6aa19-142">Una vez hecho esto, el bloc de notas volverá a abrirse correctamente en Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="6aa19-142">Una vez hecho esto, el bloc de notas volverá a abrirse correctamente en Azure Machine Learning Workbench.</span></span>
- <span data-ttu-id="6aa19-143">Todos los scripts que se proporcionan en este ejemplo tienen que ejecutarse de forma local y no en entornos como, por ejemplo, un entorno remoto de docker.</span><span class="sxs-lookup"><span data-stu-id="6aa19-143">Todos los scripts que se proporcionan en este ejemplo tienen que ejecutarse de forma local y no en entornos como, por ejemplo, un entorno remoto de docker.</span></span> <span data-ttu-id="6aa19-144">Todos los blocs de notas tienen que ejecutarse con el kernel establecido como un kernel del proyecto local denominado "PROJECTNAME local" (por ejemplo, "myImgClassUsingCNTK local").</span><span class="sxs-lookup"><span data-stu-id="6aa19-144">Todos los blocs de notas tienen que ejecutarse con el kernel establecido como un kernel del proyecto local denominado "PROJECTNAME local" (por ejemplo, "myImgClassUsingCNTK local").</span></span>

    
## <a name="create-a-new-workbench-project"></a><span data-ttu-id="6aa19-145">Creación de un nuevo proyecto de Workbench</span><span class="sxs-lookup"><span data-stu-id="6aa19-145">Creación de un nuevo proyecto de Workbench</span></span>

<span data-ttu-id="6aa19-146">Para crear un proyecto usando este ejemplo como plantilla:</span><span class="sxs-lookup"><span data-stu-id="6aa19-146">Para crear un proyecto usando este ejemplo como plantilla:</span></span>
1.  <span data-ttu-id="6aa19-147">Abra Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="6aa19-147">Abra Azure Machine Learning Workbench.</span></span>
2.  <span data-ttu-id="6aa19-148">En la página **Proyectos**, haga clic en el signo **+** y seleccione **Nuevo proyecto**.</span><span class="sxs-lookup"><span data-stu-id="6aa19-148">En la página **Proyectos**, haga clic en el signo **+** y seleccione **Nuevo proyecto**.</span></span>
3.  <span data-ttu-id="6aa19-149">En el panel **Crear nuevo proyecto**, rellene la información del proyecto nuevo.</span><span class="sxs-lookup"><span data-stu-id="6aa19-149">En el panel **Crear nuevo proyecto**, rellene la información del proyecto nuevo.</span></span>
4.  <span data-ttu-id="6aa19-150">En el cuadro de búsqueda **Buscar plantillas de proyecto**, escriba "Image Classification" (Clasificación de imágenes) y seleccione la plantilla.</span><span class="sxs-lookup"><span data-stu-id="6aa19-150">En el cuadro de búsqueda **Buscar plantillas de proyecto**, escriba "Image Classification" (Clasificación de imágenes) y seleccione la plantilla.</span></span>
5.  <span data-ttu-id="6aa19-151">Haga clic en **Create**(Crear).</span><span class="sxs-lookup"><span data-stu-id="6aa19-151">Haga clic en **Create**(Crear).</span></span>

<span data-ttu-id="6aa19-152">Cuando realice estos pasos, se creará la estructura del proyecto descrita abajo.</span><span class="sxs-lookup"><span data-stu-id="6aa19-152">Cuando realice estos pasos, se creará la estructura del proyecto descrita abajo.</span></span> <span data-ttu-id="6aa19-153">El directorio del proyecto tiene una limitación de tamaño máximo de 25 MB, ya que Azure Machine Learning Workbench crea una copia de esta carpeta después de cada ejecución (para habilitar el historial de ejecución).</span><span class="sxs-lookup"><span data-stu-id="6aa19-153">El directorio del proyecto tiene una limitación de tamaño máximo de 25 MB, ya que Azure Machine Learning Workbench crea una copia de esta carpeta después de cada ejecución (para habilitar el historial de ejecución).</span></span> <span data-ttu-id="6aa19-154">Por lo tanto, todos los archivos temporales y de imagen se guardan en el directorio *~/Desktop/imgClassificationUsingCntk_data* (en este documento lo llamaremos *DATA_DIR*).</span><span class="sxs-lookup"><span data-stu-id="6aa19-154">Por lo tanto, todos los archivos temporales y de imagen se guardan en el directorio *~/Desktop/imgClassificationUsingCntk_data* (en este documento lo llamaremos *DATA_DIR*).</span></span>

  <span data-ttu-id="6aa19-155">Carpeta</span><span class="sxs-lookup"><span data-stu-id="6aa19-155">Carpeta</span></span>| <span data-ttu-id="6aa19-156">DESCRIPCIÓN</span><span class="sxs-lookup"><span data-stu-id="6aa19-156">DESCRIPCIÓN</span></span>
  ---|---
  <span data-ttu-id="6aa19-157">aml_config/</span><span class="sxs-lookup"><span data-stu-id="6aa19-157">aml_config/</span></span>|                           <span data-ttu-id="6aa19-158">Directorio que contiene los archivos de configuración de Azure Machine Learning Workbench</span><span class="sxs-lookup"><span data-stu-id="6aa19-158">Directorio que contiene los archivos de configuración de Azure Machine Learning Workbench</span></span>
  <span data-ttu-id="6aa19-159">libraries/</span><span class="sxs-lookup"><span data-stu-id="6aa19-159">libraries/</span></span>|                              <span data-ttu-id="6aa19-160">Directorio que contiene todas las funciones auxiliares de Python y Jupyter</span><span class="sxs-lookup"><span data-stu-id="6aa19-160">Directorio que contiene todas las funciones auxiliares de Python y Jupyter</span></span>
  <span data-ttu-id="6aa19-161">notebooks/</span><span class="sxs-lookup"><span data-stu-id="6aa19-161">notebooks/</span></span>|                              <span data-ttu-id="6aa19-162">Directorio que contiene todos los blocs de notas</span><span class="sxs-lookup"><span data-stu-id="6aa19-162">Directorio que contiene todos los blocs de notas</span></span>
  <span data-ttu-id="6aa19-163">resources/</span><span class="sxs-lookup"><span data-stu-id="6aa19-163">resources/</span></span>|                              <span data-ttu-id="6aa19-164">Directorio que contiene todos los recursos (por ejemplo, direcciones URL de imágenes de moda)</span><span class="sxs-lookup"><span data-stu-id="6aa19-164">Directorio que contiene todos los recursos (por ejemplo, direcciones URL de imágenes de moda)</span></span>
  <span data-ttu-id="6aa19-165">scripts/</span><span class="sxs-lookup"><span data-stu-id="6aa19-165">scripts/</span></span>|                              <span data-ttu-id="6aa19-166">Directorio que contiene todos los scripts</span><span class="sxs-lookup"><span data-stu-id="6aa19-166">Directorio que contiene todos los scripts</span></span>
  <span data-ttu-id="6aa19-167">PARAMETERS.py</span><span class="sxs-lookup"><span data-stu-id="6aa19-167">PARAMETERS.py</span></span>|                       <span data-ttu-id="6aa19-168">Script de Python donde se especifican todos los parámetros</span><span class="sxs-lookup"><span data-stu-id="6aa19-168">Script de Python donde se especifican todos los parámetros</span></span>
  <span data-ttu-id="6aa19-169">readme.md</span><span class="sxs-lookup"><span data-stu-id="6aa19-169">readme.md</span></span>|                           <span data-ttu-id="6aa19-170">El presente documento Léame</span><span class="sxs-lookup"><span data-stu-id="6aa19-170">El presente documento Léame</span></span>


## <a name="data-description"></a><span data-ttu-id="6aa19-171">Descripción de los datos</span><span class="sxs-lookup"><span data-stu-id="6aa19-171">Descripción de los datos</span></span>

<span data-ttu-id="6aa19-172">En este tutorial se usa como ejemplo un conjunto de datos con 428 imágenes de texturas de partes de arriba de ropa.</span><span class="sxs-lookup"><span data-stu-id="6aa19-172">En este tutorial se usa como ejemplo un conjunto de datos con 428 imágenes de texturas de partes de arriba de ropa.</span></span> <span data-ttu-id="6aa19-173">Cada imagen está anotada como perteneciente a una de tres posibles texturas distintas (lunares, rayas o leopardo).</span><span class="sxs-lookup"><span data-stu-id="6aa19-173">Cada imagen está anotada como perteneciente a una de tres posibles texturas distintas (lunares, rayas o leopardo).</span></span> <span data-ttu-id="6aa19-174">Hemos usado un número de imágenes reducido para que este tutorial sea rápido de ejecutar, pero el código está totalmente probado y funciona con miles de imágenes o más.</span><span class="sxs-lookup"><span data-stu-id="6aa19-174">Hemos usado un número de imágenes reducido para que este tutorial sea rápido de ejecutar, pero el código está totalmente probado y funciona con miles de imágenes o más.</span></span> <span data-ttu-id="6aa19-175">Todas las imágenes se extrajeron con búsquedas de imágenes de Bing y se anotaron a mano tal y como se explica en la [parte 3](#using-a-custom-dataset).</span><span class="sxs-lookup"><span data-stu-id="6aa19-175">Todas las imágenes se extrajeron con búsquedas de imágenes de Bing y se anotaron a mano tal y como se explica en la [parte 3](#using-a-custom-dataset).</span></span> <span data-ttu-id="6aa19-176">Las direcciones URL de las imágenes aparecen en el archivo */resources/fashionTextureUrls.tsv* junto con sus correspondientes atributos.</span><span class="sxs-lookup"><span data-stu-id="6aa19-176">Las direcciones URL de las imágenes aparecen en el archivo */resources/fashionTextureUrls.tsv* junto con sus correspondientes atributos.</span></span>

<span data-ttu-id="6aa19-177">El script `0_downloadData.py` descarga todas las imágenes en el directorio *DATA_DIR/images/fashionTexture/*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-177">El script `0_downloadData.py` descarga todas las imágenes en el directorio *DATA_DIR/images/fashionTexture/*.</span></span> <span data-ttu-id="6aa19-178">Es probable que los vínculos de algunas de las 428 direcciones URL estén rotos.</span><span class="sxs-lookup"><span data-stu-id="6aa19-178">Es probable que los vínculos de algunas de las 428 direcciones URL estén rotos.</span></span> <span data-ttu-id="6aa19-179">Esto no supone un problema y simplemente significa que dispondremos de menos imágenes con las que entrenar y probar.</span><span class="sxs-lookup"><span data-stu-id="6aa19-179">Esto no supone un problema y simplemente significa que dispondremos de menos imágenes con las que entrenar y probar.</span></span> <span data-ttu-id="6aa19-180">Todos los scripts que se proporcionan en este ejemplo tienen que ejecutarse de forma local y no en entornos como, por ejemplo, un entorno remoto de docker.</span><span class="sxs-lookup"><span data-stu-id="6aa19-180">Todos los scripts que se proporcionan en este ejemplo tienen que ejecutarse de forma local y no en entornos como, por ejemplo, un entorno remoto de docker.</span></span>

<span data-ttu-id="6aa19-181">En la siguiente ilustración se muestran ejemplos de los atributos de lunares (izquierda), a rayas (centro) y leopardo (derecha).</span><span class="sxs-lookup"><span data-stu-id="6aa19-181">En la siguiente ilustración se muestran ejemplos de los atributos de lunares (izquierda), a rayas (centro) y leopardo (derecha).</span></span> <span data-ttu-id="6aa19-182">Se realizaron las anotaciones pertinentes en función de la prenda de ropa.</span><span class="sxs-lookup"><span data-stu-id="6aa19-182">Se realizaron las anotaciones pertinentes en función de la prenda de ropa.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/examples_all.jpg"  alt="alt text" width="700">
</p>


## <a name="part-1---model-training-and-evaluation"></a><span data-ttu-id="6aa19-183">Parte 1: Entrenamiento y evaluación del modelo</span><span class="sxs-lookup"><span data-stu-id="6aa19-183">Parte 1: Entrenamiento y evaluación del modelo</span></span>

<span data-ttu-id="6aa19-184">En la primera parte de este tutorial, entrenaremos un sistema que usa (pero no modifica) una red neuronal profunda ya entrenada.</span><span class="sxs-lookup"><span data-stu-id="6aa19-184">En la primera parte de este tutorial, entrenaremos un sistema que usa (pero no modifica) una red neuronal profunda ya entrenada.</span></span> <span data-ttu-id="6aa19-185">Esta DNN ya entrenada se usa para la captura de características, y se entrena una SVM lineal para predecir el atributo (esto es, lunares, rayas o leopardo) de una imagen determinada.</span><span class="sxs-lookup"><span data-stu-id="6aa19-185">Esta DNN ya entrenada se usa para la captura de características, y se entrena una SVM lineal para predecir el atributo (esto es, lunares, rayas o leopardo) de una imagen determinada.</span></span>

<span data-ttu-id="6aa19-186">Ahora pasaremos a describir este método detalladamente y a indicar los scripts que hay que ejecutar.</span><span class="sxs-lookup"><span data-stu-id="6aa19-186">Ahora pasaremos a describir este método detalladamente y a indicar los scripts que hay que ejecutar.</span></span> <span data-ttu-id="6aa19-187">Es recomendable que, después de cada paso, se inspeccione qué archivos se han escrito y dónde se han escrito.</span><span class="sxs-lookup"><span data-stu-id="6aa19-187">Es recomendable que, después de cada paso, se inspeccione qué archivos se han escrito y dónde se han escrito.</span></span>

<span data-ttu-id="6aa19-188">Todos los parámetros importantes (además de una breve explicación) se especifican en un único lugar: el archivo `PARAMETERS.py`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-188">Todos los parámetros importantes (además de una breve explicación) se especifican en un único lugar: el archivo `PARAMETERS.py`.</span></span>




### <a name="step-1-data-preparation"></a><span data-ttu-id="6aa19-189">Paso 1: Preparación de los datos</span><span class="sxs-lookup"><span data-stu-id="6aa19-189">Paso 1: Preparación de los datos</span></span>
`Script: 1_prepareData.py. Notebook: showImages.ipynb`

<span data-ttu-id="6aa19-190">El bloc de notas `showImages.ipynb` se puede usar para ver las imágenes y para corregir su anotación según sea necesario.</span><span class="sxs-lookup"><span data-stu-id="6aa19-190">El bloc de notas `showImages.ipynb` se puede usar para ver las imágenes y para corregir su anotación según sea necesario.</span></span> <span data-ttu-id="6aa19-191">Para ejecutar el bloc de notas, ábralo en Azure Machine Learning Workbench y haga clic en "Start Notebook Server" (Iniciar el servidor del bloc de notas) si esta opción aparece; a continuación, cambie al kernel del proyecto local denominado "PROJECTNAME local" (por ejemplo,.</span><span class="sxs-lookup"><span data-stu-id="6aa19-191">Para ejecutar el bloc de notas, ábralo en Azure Machine Learning Workbench y haga clic en "Start Notebook Server" (Iniciar el servidor del bloc de notas) si esta opción aparece; a continuación, cambie al kernel del proyecto local denominado "PROJECTNAME local" (por ejemplo,.</span></span> <span data-ttu-id="6aa19-192">"myImgClassUsingCNTK local") y ejecute todas las celdas del bloc de notas.</span><span class="sxs-lookup"><span data-stu-id="6aa19-192">"myImgClassUsingCNTK local") y ejecute todas las celdas del bloc de notas.</span></span> <span data-ttu-id="6aa19-193">Vea la sección Solución de problemas de este documento si se produce un error que indica que el bloc de notas es demasiado grande para mostrarse.</span><span class="sxs-lookup"><span data-stu-id="6aa19-193">Vea la sección Solución de problemas de este documento si se produce un error que indica que el bloc de notas es demasiado grande para mostrarse.</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/notebook_showImages.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="6aa19-194">Ahora, ejecute el script `1_prepareData.py`, que asigna todas las imágenes al conjunto de entrenamiento o al conjunto de prueba.</span><span class="sxs-lookup"><span data-stu-id="6aa19-194">Ahora, ejecute el script `1_prepareData.py`, que asigna todas las imágenes al conjunto de entrenamiento o al conjunto de prueba.</span></span> <span data-ttu-id="6aa19-195">Estas asignaciones son excluyentes entre sí; es decir, ninguna imagen de entrenamiento se usa para las pruebas y viceversa.</span><span class="sxs-lookup"><span data-stu-id="6aa19-195">Estas asignaciones son excluyentes entre sí; es decir, ninguna imagen de entrenamiento se usa para las pruebas y viceversa.</span></span> <span data-ttu-id="6aa19-196">Un 75 % de las imágenes de cada tipo se asigna de forma predeterminada y aleatoria al entrenamiento y el 25 por ciento restante, a las pruebas.</span><span class="sxs-lookup"><span data-stu-id="6aa19-196">Un 75 % de las imágenes de cada tipo se asigna de forma predeterminada y aleatoria al entrenamiento y el 25 por ciento restante, a las pruebas.</span></span> <span data-ttu-id="6aa19-197">Todos los datos generados por el script se guardan en la carpeta *DATA_DIR/proc/fashionTexture/*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-197">Todos los datos generados por el script se guardan en la carpeta *DATA_DIR/proc/fashionTexture/*.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_1_white.jpg" alt="alt text" width="700"/>
</p>



### <a name="step-2-refining-the-deep-neural-network"></a><span data-ttu-id="6aa19-198">Paso 2: Perfeccionar la red neuronal profunda</span><span class="sxs-lookup"><span data-stu-id="6aa19-198">Paso 2: Perfeccionar la red neuronal profunda</span></span>
`Script: 2_refineDNN.py`

<span data-ttu-id="6aa19-199">Como ya hemos explicado en la parte 1 de este tutorial, la DNN ya entrenada se mantiene inamovible (es decir, no se perfecciona).</span><span class="sxs-lookup"><span data-stu-id="6aa19-199">Como ya hemos explicado en la parte 1 de este tutorial, la DNN ya entrenada se mantiene inamovible (es decir, no se perfecciona).</span></span> <span data-ttu-id="6aa19-200">Pero el script `2_refineDNN.py` se ejecuta en la parte 1, dado que carga un modelo de [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) ya entrenado [2] y lo modifica (para, por ejemplo, permitir mayores resoluciones de imagen de entrada).</span><span class="sxs-lookup"><span data-stu-id="6aa19-200">Pero el script `2_refineDNN.py` se ejecuta en la parte 1, dado que carga un modelo de [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) ya entrenado [2] y lo modifica (para, por ejemplo, permitir mayores resoluciones de imagen de entrada).</span></span> <span data-ttu-id="6aa19-201">Este paso es rápido (tarda apenas segundos) y no requiere una GPU.</span><span class="sxs-lookup"><span data-stu-id="6aa19-201">Este paso es rápido (tarda apenas segundos) y no requiere una GPU.</span></span>

<span data-ttu-id="6aa19-202">En la parte 2 del tutorial, una modificación en el archivo PARAMETERS.py hace que el script `2_refineDNN.py` también perfeccione la DNN ya entrenada.</span><span class="sxs-lookup"><span data-stu-id="6aa19-202">En la parte 2 del tutorial, una modificación en el archivo PARAMETERS.py hace que el script `2_refineDNN.py` también perfeccione la DNN ya entrenada.</span></span> <span data-ttu-id="6aa19-203">Durante este perfeccionamiento, se ejecutan 45 tandas de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="6aa19-203">Durante este perfeccionamiento, se ejecutan 45 tandas de entrenamiento.</span></span>

<span data-ttu-id="6aa19-204">En uno y otro caso, el modelo final se escribe en el archivo *DATA_DIR/proc/fashionTexture/cntk_fixed.model*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-204">En uno y otro caso, el modelo final se escribe en el archivo *DATA_DIR/proc/fashionTexture/cntk_fixed.model*.</span></span>

### <a name="step-3-evaluate-dnn-for-all-images"></a><span data-ttu-id="6aa19-205">Paso 3: Evaluar la DNN para todas las imágenes</span><span class="sxs-lookup"><span data-stu-id="6aa19-205">Paso 3: Evaluar la DNN para todas las imágenes</span></span>
`Script: 3_runDNN.py`

<span data-ttu-id="6aa19-206">Ahora, podemos usar la DNN (posiblemente mejorada) del último paso para caracterizar nuestras imágenes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-206">Ahora, podemos usar la DNN (posiblemente mejorada) del último paso para caracterizar nuestras imágenes.</span></span> <span data-ttu-id="6aa19-207">Dada una imagen como entrada de la DNN, la salida es el vector de 512 flotantes de la penúltima capa del modelo.</span><span class="sxs-lookup"><span data-stu-id="6aa19-207">Dada una imagen como entrada de la DNN, la salida es el vector de 512 flotantes de la penúltima capa del modelo.</span></span> <span data-ttu-id="6aa19-208">Este vector tiene unas dimensiones mucho más pequeñas que la propia imagen.</span><span class="sxs-lookup"><span data-stu-id="6aa19-208">Este vector tiene unas dimensiones mucho más pequeñas que la propia imagen.</span></span> <span data-ttu-id="6aa19-209">A pesar de ello, debe contener (y resaltar incluso) toda la información de la imagen que sea relevante para identificar el atributo de la imagen (esto es, si la prenda de ropa es de lunares, a rayas o con estampado de leopardo).</span><span class="sxs-lookup"><span data-stu-id="6aa19-209">A pesar de ello, debe contener (y resaltar incluso) toda la información de la imagen que sea relevante para identificar el atributo de la imagen (esto es, si la prenda de ropa es de lunares, a rayas o con estampado de leopardo).</span></span>

<span data-ttu-id="6aa19-210">Todas las representaciones de imagen de la DNN se guardan en el archivo *DATA_DIR/proc/fashionTexture/cntkFiles/features.pickle*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-210">Todas las representaciones de imagen de la DNN se guardan en el archivo *DATA_DIR/proc/fashionTexture/cntkFiles/features.pickle*.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_4_white.jpg" alt="alt text" width="700"/>
</p>


### <a name="step-4-support-vector-machine-training"></a><span data-ttu-id="6aa19-211">Paso 4: Entrenamiento de una máquina de vectores de soporte</span><span class="sxs-lookup"><span data-stu-id="6aa19-211">Paso 4: Entrenamiento de una máquina de vectores de soporte</span></span>
`Script: 4_trainSVM.py`

<span data-ttu-id="6aa19-212">La representación de 512 flotantes calculada en el último paso se usa para entrenar un clasificador de SVM: dada una imagen como entrada, la SVM genera una puntuación para cada atributo que esté presente.</span><span class="sxs-lookup"><span data-stu-id="6aa19-212">La representación de 512 flotantes calculada en el último paso se usa para entrenar un clasificador de SVM: dada una imagen como entrada, la SVM genera una puntuación para cada atributo que esté presente.</span></span> <span data-ttu-id="6aa19-213">En nuestro conjunto de datos de ejemplo, esto significa una puntuación para "rayas", para "lunares" y para "leopardo".</span><span class="sxs-lookup"><span data-stu-id="6aa19-213">En nuestro conjunto de datos de ejemplo, esto significa una puntuación para "rayas", para "lunares" y para "leopardo".</span></span>

<span data-ttu-id="6aa19-214">El script `4_trainSVM.py` carga las imágenes de entrenamiento, entrena una SVM para los distintos valores del parámetro de regularización (margen de demora) C y mantiene la SVM con la máxima precisión.</span><span class="sxs-lookup"><span data-stu-id="6aa19-214">El script `4_trainSVM.py` carga las imágenes de entrenamiento, entrena una SVM para los distintos valores del parámetro de regularización (margen de demora) C y mantiene la SVM con la máxima precisión.</span></span> <span data-ttu-id="6aa19-215">La precisión de la clasificación se imprime en la consola y se traza en Workbench.</span><span class="sxs-lookup"><span data-stu-id="6aa19-215">La precisión de la clasificación se imprime en la consola y se traza en Workbench.</span></span> <span data-ttu-id="6aa19-216">En el caso de los datos de textura proporcionados, estos valores deben rondar el 100 % y el 88 % respectivamente.</span><span class="sxs-lookup"><span data-stu-id="6aa19-216">En el caso de los datos de textura proporcionados, estos valores deben rondar el 100 % y el 88 % respectivamente.</span></span> <span data-ttu-id="6aa19-217">Por último, la SVM entrenada se escribe en el archivo *DATA_DIR/proc/fashionTexture/cntkFiles/svm.np*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-217">Por último, la SVM entrenada se escribe en el archivo *DATA_DIR/proc/fashionTexture/cntkFiles/svm.np*.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/vienna_svm_log_zoom.jpg" alt="alt text" width="700"/>
</p>



### <a name="step-5-evaluation-and-visualization"></a><span data-ttu-id="6aa19-218">Paso 5: Evaluación y visualización</span><span class="sxs-lookup"><span data-stu-id="6aa19-218">Paso 5: Evaluación y visualización</span></span>
`Script: 5_evaluate.py. Notebook: showResults.ipynb`

<span data-ttu-id="6aa19-219">La precisión del clasificador de imágenes entrenado se puede medir con el script `5_evaluate.py`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-219">La precisión del clasificador de imágenes entrenado se puede medir con el script `5_evaluate.py`.</span></span> <span data-ttu-id="6aa19-220">El script puntúa todas las imágenes de prueba con el clasificador de SVM entrenado, asigna a cada imagen el atributo con la puntuación más alta y compara los atributos de predicción con anotaciones precisas.</span><span class="sxs-lookup"><span data-stu-id="6aa19-220">El script puntúa todas las imágenes de prueba con el clasificador de SVM entrenado, asigna a cada imagen el atributo con la puntuación más alta y compara los atributos de predicción con anotaciones precisas.</span></span>

<span data-ttu-id="6aa19-221">Aquí mostramos la salida del script `5_evaluate.py`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-221">Aquí mostramos la salida del script `5_evaluate.py`.</span></span> <span data-ttu-id="6aa19-222">Se calcula la precisión de la clasificación de cada clase individual, así como la precisión del conjunto de prueba completo ("precisión total") y el promedio de las precisiones individuales ("promedio total de precisión de clases").</span><span class="sxs-lookup"><span data-stu-id="6aa19-222">Se calcula la precisión de la clasificación de cada clase individual, así como la precisión del conjunto de prueba completo ("precisión total") y el promedio de las precisiones individuales ("promedio total de precisión de clases").</span></span> <span data-ttu-id="6aa19-223">100 % corresponde a la mejor precisión posible y 0 %, a la peor.</span><span class="sxs-lookup"><span data-stu-id="6aa19-223">100 % corresponde a la mejor precisión posible y 0 %, a la peor.</span></span> <span data-ttu-id="6aa19-224">Una estimación aleatoria generaría un promedio total de precisión de clases de 1 sobre el número de atributos: en nuestro caso, esta precisión sería pues del 33,33 %.</span><span class="sxs-lookup"><span data-stu-id="6aa19-224">Una estimación aleatoria generaría un promedio total de precisión de clases de 1 sobre el número de atributos: en nuestro caso, esta precisión sería pues del 33,33 %.</span></span> <span data-ttu-id="6aa19-225">Estos resultados mejoran significativamente cuando se usa una mayor resolución de entrada, como `rf_inputResoluton = 1000`, si bien esto conlleva tiempos de cálculo de DNN más prolongados.</span><span class="sxs-lookup"><span data-stu-id="6aa19-225">Estos resultados mejoran significativamente cuando se usa una mayor resolución de entrada, como `rf_inputResoluton = 1000`, si bien esto conlleva tiempos de cálculo de DNN más prolongados.</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_6_white.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="6aa19-226">Además de la precisión, la curva ROC se traza con la correspondiente área bajo la curva (izquierda).</span><span class="sxs-lookup"><span data-stu-id="6aa19-226">Además de la precisión, la curva ROC se traza con la correspondiente área bajo la curva (izquierda).</span></span> <span data-ttu-id="6aa19-227">También mostramos la matriz de confusión (derecha):</span><span class="sxs-lookup"><span data-stu-id="6aa19-227">También mostramos la matriz de confusión (derecha):</span></span>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/roc_confMat.jpg" alt="alt text" width="700"/>
</p>

<span data-ttu-id="6aa19-228">Por último, se proporciona el bloc de notas `showResults.py` para desplazarse por las imágenes de prueba y ver las correspondientes puntuaciones de clasificación.</span><span class="sxs-lookup"><span data-stu-id="6aa19-228">Por último, se proporciona el bloc de notas `showResults.py` para desplazarse por las imágenes de prueba y ver las correspondientes puntuaciones de clasificación.</span></span> <span data-ttu-id="6aa19-229">Tal como se explica en el paso 1, cada bloc de notas de este ejemplo debe usar el kernel del proyecto local denominado "PROJECTNAME local":</span><span class="sxs-lookup"><span data-stu-id="6aa19-229">Tal como se explica en el paso 1, cada bloc de notas de este ejemplo debe usar el kernel del proyecto local denominado "PROJECTNAME local":</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/notebook_showResults.jpg" alt="alt text" width="700"/>
</p>





### <a name="step-6-deployment"></a><span data-ttu-id="6aa19-230">Paso 6: Implementación</span><span class="sxs-lookup"><span data-stu-id="6aa19-230">Paso 6: Implementación</span></span>
`Scripts: 6_callWebservice.py, deploymain.py. Notebook: deploy.ipynb`

<span data-ttu-id="6aa19-231">El sistema entrenado ya se puede publicar como una API de REST.</span><span class="sxs-lookup"><span data-stu-id="6aa19-231">El sistema entrenado ya se puede publicar como una API de REST.</span></span> <span data-ttu-id="6aa19-232">La implementación se explica en el bloc de notas `deploy.ipynb` y se basa en la funcionalidad de Azure Machine Learning Workbench (recuerde establecer como kernel el kernel del proyecto local denominado "PROJECTNAME local").</span><span class="sxs-lookup"><span data-stu-id="6aa19-232">La implementación se explica en el bloc de notas `deploy.ipynb` y se basa en la funcionalidad de Azure Machine Learning Workbench (recuerde establecer como kernel el kernel del proyecto local denominado "PROJECTNAME local").</span></span> <span data-ttu-id="6aa19-233">Si quiere obtener más información detallada de la implementación, consulte también la excelente sección de implementación del [tutorial de IRIS](tutorial-classifying-iris-part-3.md).</span><span class="sxs-lookup"><span data-stu-id="6aa19-233">Si quiere obtener más información detallada de la implementación, consulte también la excelente sección de implementación del [tutorial de IRIS](tutorial-classifying-iris-part-3.md).</span></span>

<span data-ttu-id="6aa19-234">Una vez implementado, el servicio web se puede llamar con el script `6_callWebservice.py`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-234">Una vez implementado, el servicio web se puede llamar con el script `6_callWebservice.py`.</span></span> <span data-ttu-id="6aa19-235">Cabe mencionar que la dirección IP (ya sea local o en la nube) del servicio web debe establecerse antes en el script.</span><span class="sxs-lookup"><span data-stu-id="6aa19-235">Cabe mencionar que la dirección IP (ya sea local o en la nube) del servicio web debe establecerse antes en el script.</span></span> <span data-ttu-id="6aa19-236">En el bloc de notas `deploy.ipynb` se explica cómo encontrar esta dirección IP.</span><span class="sxs-lookup"><span data-stu-id="6aa19-236">En el bloc de notas `deploy.ipynb` se explica cómo encontrar esta dirección IP.</span></span>








## <a name="part-2---accuracy-improvements"></a><span data-ttu-id="6aa19-237">Parte 2: Mejoras en la precisión</span><span class="sxs-lookup"><span data-stu-id="6aa19-237">Parte 2: Mejoras en la precisión</span></span>

<span data-ttu-id="6aa19-238">En la parte 1 hemos visto cómo clasificar una imagen, entrenando para ello una máquina de vectores de soporte lineal en la salida de 512 flotantes de una red neuronal profunda.</span><span class="sxs-lookup"><span data-stu-id="6aa19-238">En la parte 1 hemos visto cómo clasificar una imagen, entrenando para ello una máquina de vectores de soporte lineal en la salida de 512 flotantes de una red neuronal profunda.</span></span> <span data-ttu-id="6aa19-239">Esta DNN se entrenó previamente con millones de imágenes y la penúltima capa se devolvió como un vector de características.</span><span class="sxs-lookup"><span data-stu-id="6aa19-239">Esta DNN se entrenó previamente con millones de imágenes y la penúltima capa se devolvió como un vector de características.</span></span> <span data-ttu-id="6aa19-240">Este método es rápido, porque la DNN se usa tal cual y, con todo, da buenos resultados con frecuencia.</span><span class="sxs-lookup"><span data-stu-id="6aa19-240">Este método es rápido, porque la DNN se usa tal cual y, con todo, da buenos resultados con frecuencia.</span></span>

<span data-ttu-id="6aa19-241">Ahora nos centraremos en diversas maneras de mejorar la precisión del modelo de la parte 1.</span><span class="sxs-lookup"><span data-stu-id="6aa19-241">Ahora nos centraremos en diversas maneras de mejorar la precisión del modelo de la parte 1.</span></span> <span data-ttu-id="6aa19-242">En concreto, vamos a perfeccionar la DNN en lugar mantenerla inamovible.</span><span class="sxs-lookup"><span data-stu-id="6aa19-242">En concreto, vamos a perfeccionar la DNN en lugar mantenerla inamovible.</span></span>

### <a name="dnn-refinement"></a><span data-ttu-id="6aa19-243">Perfeccionamiento de la DNN</span><span class="sxs-lookup"><span data-stu-id="6aa19-243">Perfeccionamiento de la DNN</span></span>

<span data-ttu-id="6aa19-244">En lugar de en una SVM, la clasificación se puede realizar directamente en la red neuronal.</span><span class="sxs-lookup"><span data-stu-id="6aa19-244">En lugar de en una SVM, la clasificación se puede realizar directamente en la red neuronal.</span></span> <span data-ttu-id="6aa19-245">Esto se logra agregando una nueva última capa a la DNN ya entrenada, que toma los 512 flotantes de la penúltima capa como entrada.</span><span class="sxs-lookup"><span data-stu-id="6aa19-245">Esto se logra agregando una nueva última capa a la DNN ya entrenada, que toma los 512 flotantes de la penúltima capa como entrada.</span></span> <span data-ttu-id="6aa19-246">La ventaja de realizar la clasificación en la DNN es que ahora la red entera se puede volver a entrenar con una nueva propagación.</span><span class="sxs-lookup"><span data-stu-id="6aa19-246">La ventaja de realizar la clasificación en la DNN es que ahora la red entera se puede volver a entrenar con una nueva propagación.</span></span> <span data-ttu-id="6aa19-247">Con este método se suele obtener una precisión de la clasificación mucho mejor a si usáramos la DNN ya entrenada tal cual, pero conlleva unos tiempos de cálculo de DNN más prolongados (incluso con una GPU).</span><span class="sxs-lookup"><span data-stu-id="6aa19-247">Con este método se suele obtener una precisión de la clasificación mucho mejor a si usáramos la DNN ya entrenada tal cual, pero conlleva unos tiempos de cálculo de DNN más prolongados (incluso con una GPU).</span></span>

<span data-ttu-id="6aa19-248">Entrenar la red neuronal en lugar de una SVM se efectúa cambiando la variable `classifier` en `PARAMETERS.py` de `svm` a `dnn`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-248">Entrenar la red neuronal en lugar de una SVM se efectúa cambiando la variable `classifier` en `PARAMETERS.py` de `svm` a `dnn`.</span></span> <span data-ttu-id="6aa19-249">Luego, tal y como se describe en la parte 1, hay que volver a ejecutar todos los scripts, excepto el de preparación de los datos (paso 1) y el de entrenamiento de SVM (paso 4).</span><span class="sxs-lookup"><span data-stu-id="6aa19-249">Luego, tal y como se describe en la parte 1, hay que volver a ejecutar todos los scripts, excepto el de preparación de los datos (paso 1) y el de entrenamiento de SVM (paso 4).</span></span> <span data-ttu-id="6aa19-250">El perfeccionamiento de la DNN requiere una GPU.</span><span class="sxs-lookup"><span data-stu-id="6aa19-250">El perfeccionamiento de la DNN requiere una GPU.</span></span> <span data-ttu-id="6aa19-251">Si no existe ninguna GPU o si la GPU está bloqueada (por ejemplo, porque haya una ejecución de CNTK anterior), el script `2_refineDNN.py` generará un error.</span><span class="sxs-lookup"><span data-stu-id="6aa19-251">Si no existe ninguna GPU o si la GPU está bloqueada (por ejemplo, porque haya una ejecución de CNTK anterior), el script `2_refineDNN.py` generará un error.</span></span> <span data-ttu-id="6aa19-252">En algunas GPU, el entrenamiento de la DNN puede producir un error de memoria insuficiente, que se puede soslayar si se reduce el tamaño del minilote (variable `cntk_mb_size` en `PARAMETERS.py`).</span><span class="sxs-lookup"><span data-stu-id="6aa19-252">En algunas GPU, el entrenamiento de la DNN puede producir un error de memoria insuficiente, que se puede soslayar si se reduce el tamaño del minilote (variable `cntk_mb_size` en `PARAMETERS.py`).</span></span>

<span data-ttu-id="6aa19-253">Una vez completado el entrenamiento, el modelo perfeccionado se guarda en *DATA_DIR/proc/fashionTexture/cntk_refined.model* y se traza un gráfico que muestra cómo varían los errores de clasificación de entrenamiento y de prueba durante el entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="6aa19-253">Una vez completado el entrenamiento, el modelo perfeccionado se guarda en *DATA_DIR/proc/fashionTexture/cntk_refined.model* y se traza un gráfico que muestra cómo varían los errores de clasificación de entrenamiento y de prueba durante el entrenamiento.</span></span> <span data-ttu-id="6aa19-254">Observe en ese gráfico que el error del conjunto de entrenamiento es mucho menor que el del conjunto de prueba.</span><span class="sxs-lookup"><span data-stu-id="6aa19-254">Observe en ese gráfico que el error del conjunto de entrenamiento es mucho menor que el del conjunto de prueba.</span></span> <span data-ttu-id="6aa19-255">Este comportamiento, conocido como sobreajuste, se puede minimizar, por ejemplo, usando un valor más alto como tasa de eliminación (`rf_dropoutRate`).</span><span class="sxs-lookup"><span data-stu-id="6aa19-255">Este comportamiento, conocido como sobreajuste, se puede minimizar, por ejemplo, usando un valor más alto como tasa de eliminación (`rf_dropoutRate`).</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/output_script_3_plot.png" alt="alt text" height="300"/>
</p>

<span data-ttu-id="6aa19-256">Como se aprecia en el siguiente gráfico, la precisión del conjunto de datos proporcionado usando el perfeccionamiento de la DNN es del 92,35 % en comparación con el 88,92 % previo (parte 1).</span><span class="sxs-lookup"><span data-stu-id="6aa19-256">Como se aprecia en el siguiente gráfico, la precisión del conjunto de datos proporcionado usando el perfeccionamiento de la DNN es del 92,35 % en comparación con el 88,92 % previo (parte 1).</span></span> <span data-ttu-id="6aa19-257">En concreto, las imágenes relativas a los "lunares" (dotted) mejoran considerablemente y logran un área bajo la curva de ROC de 0,98 con el perfeccionamiento, frente al 0,94 anterior.</span><span class="sxs-lookup"><span data-stu-id="6aa19-257">En concreto, las imágenes relativas a los "lunares" (dotted) mejoran considerablemente y logran un área bajo la curva de ROC de 0,98 con el perfeccionamiento, frente al 0,94 anterior.</span></span> <span data-ttu-id="6aa19-258">Estamos usando un conjunto de datos pequeño y, por lo tanto, las precisiones reales al ejecutar el código son diferentes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-258">Estamos usando un conjunto de datos pequeño y, por lo tanto, las precisiones reales al ejecutar el código son diferentes.</span></span> <span data-ttu-id="6aa19-259">Esta discrepancia obedece a efectos estocásticos como la división aleatoria de las imágenes en los conjuntos de pruebas de entrenamiento y de prueba.</span><span class="sxs-lookup"><span data-stu-id="6aa19-259">Esta discrepancia obedece a efectos estocásticos como la división aleatoria de las imágenes en los conjuntos de pruebas de entrenamiento y de prueba.</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/roc_confMat_dnn.jpg" alt="alt text" width="700"/>
</p>

### <a name="run-history-tracking"></a><span data-ttu-id="6aa19-260">Ejecutar un seguimiento del historial</span><span class="sxs-lookup"><span data-stu-id="6aa19-260">Ejecutar un seguimiento del historial</span></span>

<span data-ttu-id="6aa19-261">Azure Machine Learning Workbench almacena el historial de cada ejecución en Azure para, así, permitir que se puedan comparar dos o más ejecuciones con incluso semanas de diferencia.</span><span class="sxs-lookup"><span data-stu-id="6aa19-261">Azure Machine Learning Workbench almacena el historial de cada ejecución en Azure para, así, permitir que se puedan comparar dos o más ejecuciones con incluso semanas de diferencia.</span></span> <span data-ttu-id="6aa19-262">Esto se explica con más detalle en el [tutorial de Iris](tutorial-classifying-iris-part-2.md).</span><span class="sxs-lookup"><span data-stu-id="6aa19-262">Esto se explica con más detalle en el [tutorial de Iris](tutorial-classifying-iris-part-2.md).</span></span> <span data-ttu-id="6aa19-263">También se ilustra en las siguientes capturas de pantalla, donde comparamos dos ejecuciones del script `5_evaluate.py`, ya sea por medio del perfeccionamiento de la DNN, es decir, `classifier = "dnn"` (número de ejecución 148) o del entrenamiento de la SVM, es decir, `classifier = "svm"` (número de ejecución 150).</span><span class="sxs-lookup"><span data-stu-id="6aa19-263">También se ilustra en las siguientes capturas de pantalla, donde comparamos dos ejecuciones del script `5_evaluate.py`, ya sea por medio del perfeccionamiento de la DNN, es decir, `classifier = "dnn"` (número de ejecución 148) o del entrenamiento de la SVM, es decir, `classifier = "svm"` (número de ejecución 150).</span></span>

<span data-ttu-id="6aa19-264">En la primera captura de pantalla, el perfeccionamiento de la DNN lleva a unas mejores precisiones que con el entrenamiento de SVM en todas las clases.</span><span class="sxs-lookup"><span data-stu-id="6aa19-264">En la primera captura de pantalla, el perfeccionamiento de la DNN lleva a unas mejores precisiones que con el entrenamiento de SVM en todas las clases.</span></span> <span data-ttu-id="6aa19-265">La segunda captura de pantalla muestra todas las métricas de las que se hace un seguimiento, incluido el clasificador que se usó.</span><span class="sxs-lookup"><span data-stu-id="6aa19-265">La segunda captura de pantalla muestra todas las métricas de las que se hace un seguimiento, incluido el clasificador que se usó.</span></span> <span data-ttu-id="6aa19-266">Este seguimiento se realiza en el script `5_evaluate.py`, llamando al registrador de Azure Machine Learning Workbench.</span><span class="sxs-lookup"><span data-stu-id="6aa19-266">Este seguimiento se realiza en el script `5_evaluate.py`, llamando al registrador de Azure Machine Learning Workbench.</span></span> <span data-ttu-id="6aa19-267">El script también guarda la curva de ROC y la matriz de confusión en la carpeta *outputs*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-267">El script también guarda la curva de ROC y la matriz de confusión en la carpeta *outputs*.</span></span> <span data-ttu-id="6aa19-268">Esta carpeta *outputs* es especial, en el sentido de que también se realiza un seguimiento de su contenido por medio de la característica de historial de Workbench.</span><span class="sxs-lookup"><span data-stu-id="6aa19-268">Esta carpeta *outputs* es especial, en el sentido de que también se realiza un seguimiento de su contenido por medio de la característica de historial de Workbench.</span></span> <span data-ttu-id="6aa19-269">Por lo tanto, se puede tener acceso a sus archivos siempre que se quiera, independientemente de si las copias locales se han sobrescrito.</span><span class="sxs-lookup"><span data-stu-id="6aa19-269">Por lo tanto, se puede tener acceso a sus archivos siempre que se quiera, independientemente de si las copias locales se han sobrescrito.</span></span>

<span data-ttu-id="6aa19-270"><p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison1.jpg" alt="alt text" width="700"/> </p>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison2b.jpg" alt="alt text" width="700"/>
</p></span><span class="sxs-lookup"><span data-stu-id="6aa19-270"><p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison1.jpg" alt="alt text" width="700"/> </p>

<p align="center">
<img src="media/scenario-image-classification-using-cntk/run_comparison2b.jpg" alt="alt text" width="700"/>
</p></span></span>


### <a name="parameter-tuning"></a><span data-ttu-id="6aa19-271">Ajuste de parámetros</span><span class="sxs-lookup"><span data-stu-id="6aa19-271">Ajuste de parámetros</span></span>

<span data-ttu-id="6aa19-272">Como sucede en la mayoría de los proyectos de aprendizaje automático, para obtener buenos resultados en un conjunto de datos nuevo, es necesario ajustar los parámetros con mucho cuidado, así como sopesar detenidamente diversas decisiones de diseño.</span><span class="sxs-lookup"><span data-stu-id="6aa19-272">Como sucede en la mayoría de los proyectos de aprendizaje automático, para obtener buenos resultados en un conjunto de datos nuevo, es necesario ajustar los parámetros con mucho cuidado, así como sopesar detenidamente diversas decisiones de diseño.</span></span> <span data-ttu-id="6aa19-273">Para hacer estas tareas más sencillas, todos los parámetros importantes (además de una breve explicación) se especifican en un único lugar: el archivo `PARAMETERS.py`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-273">Para hacer estas tareas más sencillas, todos los parámetros importantes (además de una breve explicación) se especifican en un único lugar: el archivo `PARAMETERS.py`.</span></span>

<span data-ttu-id="6aa19-274">Estos son algunos aspectos que sin duda se traducirán en mejoras:</span><span class="sxs-lookup"><span data-stu-id="6aa19-274">Estos son algunos aspectos que sin duda se traducirán en mejoras:</span></span>

- <span data-ttu-id="6aa19-275">Calidad de los datos: asegúrese de que la calidad de los conjuntos de entrenamiento y de prueba es alta.</span><span class="sxs-lookup"><span data-stu-id="6aa19-275">Calidad de los datos: asegúrese de que la calidad de los conjuntos de entrenamiento y de prueba es alta.</span></span> <span data-ttu-id="6aa19-276">Es decir, procure que las imágenes estén correctamente anotadas, que ha quitado las imágenes ambiguas (por ejemplo, prendas de ropa que tienen tanto rayas como lunares) y que los atributos son excluyentes entre sí (dicho de otro modo, que los ha elegido de tal forma que cada imagen pertenece a exactamente un atributo).</span><span class="sxs-lookup"><span data-stu-id="6aa19-276">Es decir, procure que las imágenes estén correctamente anotadas, que ha quitado las imágenes ambiguas (por ejemplo, prendas de ropa que tienen tanto rayas como lunares) y que los atributos son excluyentes entre sí (dicho de otro modo, que los ha elegido de tal forma que cada imagen pertenece a exactamente un atributo).</span></span>

- <span data-ttu-id="6aa19-277">Hay constancia de que, si el objeto de interés es pequeño en la imagen, los métodos de clasificación de imágenes no funcionan bien.</span><span class="sxs-lookup"><span data-stu-id="6aa19-277">Hay constancia de que, si el objeto de interés es pequeño en la imagen, los métodos de clasificación de imágenes no funcionan bien.</span></span> <span data-ttu-id="6aa19-278">En tales casos, considere el uso de un método de detección de objetos, tal y como se describe en este [tutorial](https://github.com/Azure/ObjectDetectionUsingCntk).</span><span class="sxs-lookup"><span data-stu-id="6aa19-278">En tales casos, considere el uso de un método de detección de objetos, tal y como se describe en este [tutorial](https://github.com/Azure/ObjectDetectionUsingCntk).</span></span>
- <span data-ttu-id="6aa19-279">Perfeccionamiento de la DNN: seguramente, el parámetro más importante para realizar esta tarea correctamente sea la velocidad de aprendizaje `rf_lrPerMb`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-279">Perfeccionamiento de la DNN: seguramente, el parámetro más importante para realizar esta tarea correctamente sea la velocidad de aprendizaje `rf_lrPerMb`.</span></span> <span data-ttu-id="6aa19-280">Si la precisión del conjunto de entrenamiento (primera figura de la parte 2) no ronda el 0-5 %, probablemente se deba a un error en la velocidad de aprendizaje.</span><span class="sxs-lookup"><span data-stu-id="6aa19-280">Si la precisión del conjunto de entrenamiento (primera figura de la parte 2) no ronda el 0-5 %, probablemente se deba a un error en la velocidad de aprendizaje.</span></span> <span data-ttu-id="6aa19-281">Los demás parámetros que comienzan por `rf_` no son tan importantes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-281">Los demás parámetros que comienzan por `rf_` no son tan importantes.</span></span> <span data-ttu-id="6aa19-282">Normalmente, el error de entrenamiento debería disminuir exponencialmente y aproximarse a 0 % después del entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="6aa19-282">Normalmente, el error de entrenamiento debería disminuir exponencialmente y aproximarse a 0 % después del entrenamiento.</span></span>

- <span data-ttu-id="6aa19-283">Resolución de entrada: la resolución de imagen predeterminada es 224 x 224 píxeles.</span><span class="sxs-lookup"><span data-stu-id="6aa19-283">Resolución de entrada: la resolución de imagen predeterminada es 224 x 224 píxeles.</span></span> <span data-ttu-id="6aa19-284">Si usa una resolución de imagen mayor (parámetro: `rf_inputResoluton`), por ejemplo, 448 x 448 o 896 x 896 píxeles, con frecuencia obtendrá una mejora significativa, pero el perfeccionamiento de la DNN tardará más en completarse.</span><span class="sxs-lookup"><span data-stu-id="6aa19-284">Si usa una resolución de imagen mayor (parámetro: `rf_inputResoluton`), por ejemplo, 448 x 448 o 896 x 896 píxeles, con frecuencia obtendrá una mejora significativa, pero el perfeccionamiento de la DNN tardará más en completarse.</span></span> <span data-ttu-id="6aa19-285">**Usar una mayor resolución es inocuo y casi siempre aumenta la precisión**.</span><span class="sxs-lookup"><span data-stu-id="6aa19-285">**Usar una mayor resolución es inocuo y casi siempre aumenta la precisión**.</span></span>

- <span data-ttu-id="6aa19-286">Sobreajuste de la DNN: procure que no haya mucha diferencia entre la precisión del conjunto de entrenamiento y la del conjunto de prueba durante el perfeccionamiento de la DNN (primera figura de la parte 2).</span><span class="sxs-lookup"><span data-stu-id="6aa19-286">Sobreajuste de la DNN: procure que no haya mucha diferencia entre la precisión del conjunto de entrenamiento y la del conjunto de prueba durante el perfeccionamiento de la DNN (primera figura de la parte 2).</span></span> <span data-ttu-id="6aa19-287">Esta diferencia se puede reducir usando tasas de eliminación (`rf_dropoutRate`) de 0,5 o más, así como aumentando el peso del regularizador `rf_l2RegWeight`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-287">Esta diferencia se puede reducir usando tasas de eliminación (`rf_dropoutRate`) de 0,5 o más, así como aumentando el peso del regularizador `rf_l2RegWeight`.</span></span> <span data-ttu-id="6aa19-288">Usar una tasa de eliminación elevada puede ser especialmente útil si la resolución de imagen de entrada de DNN es alta.</span><span class="sxs-lookup"><span data-stu-id="6aa19-288">Usar una tasa de eliminación elevada puede ser especialmente útil si la resolución de imagen de entrada de DNN es alta.</span></span>

- <span data-ttu-id="6aa19-289">Pruebe a usar DNN un poco más profundas, cambiando para ello `rf_pretrainedModelFilename` de `ResNet_18.model` a `ResNet_34.model` o a `ResNet_50.model`.</span><span class="sxs-lookup"><span data-stu-id="6aa19-289">Pruebe a usar DNN un poco más profundas, cambiando para ello `rf_pretrainedModelFilename` de `ResNet_18.model` a `ResNet_34.model` o a `ResNet_50.model`.</span></span> <span data-ttu-id="6aa19-290">El modelo ResNet-50 no solo es más profundo, sino que, además, su salida de la penúltima capa tiene un tamaño de 2048 flotantes (frente a los 512 flotantes de los modelos ResNet-18 y ResNet 34).</span><span class="sxs-lookup"><span data-stu-id="6aa19-290">El modelo ResNet-50 no solo es más profundo, sino que, además, su salida de la penúltima capa tiene un tamaño de 2048 flotantes (frente a los 512 flotantes de los modelos ResNet-18 y ResNet 34).</span></span> <span data-ttu-id="6aa19-291">Esta mayor dimensión puede resultar especialmente útil al entrenar un clasificador de SVM.</span><span class="sxs-lookup"><span data-stu-id="6aa19-291">Esta mayor dimensión puede resultar especialmente útil al entrenar un clasificador de SVM.</span></span>

## <a name="part-3---custom-dataset"></a><span data-ttu-id="6aa19-292">Parte 3: Conjunto de datos personalizado</span><span class="sxs-lookup"><span data-stu-id="6aa19-292">Parte 3: Conjunto de datos personalizado</span></span>

<span data-ttu-id="6aa19-293">En las partes 1 y 2, hemos entrenado y evaluado un modelo de clasificación de imágenes usando las imágenes de texturas de partes de arriba de ropa proporcionadas.</span><span class="sxs-lookup"><span data-stu-id="6aa19-293">En las partes 1 y 2, hemos entrenado y evaluado un modelo de clasificación de imágenes usando las imágenes de texturas de partes de arriba de ropa proporcionadas.</span></span> <span data-ttu-id="6aa19-294">Ahora, describiremos cómo usar un conjunto de datos personalizado proporcionado por el usuario, en lugar de esas imágenes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-294">Ahora, describiremos cómo usar un conjunto de datos personalizado proporcionado por el usuario, en lugar de esas imágenes.</span></span> <span data-ttu-id="6aa19-295">O bien, si no hay un conjunto disponible, cómo generar y anotar uno por medio de la búsqueda de imágenes de Bing.</span><span class="sxs-lookup"><span data-stu-id="6aa19-295">O bien, si no hay un conjunto disponible, cómo generar y anotar uno por medio de la búsqueda de imágenes de Bing.</span></span>

### <a name="using-a-custom-dataset"></a><span data-ttu-id="6aa19-296">Usar un conjunto de datos personalizado</span><span class="sxs-lookup"><span data-stu-id="6aa19-296">Usar un conjunto de datos personalizado</span></span>

<span data-ttu-id="6aa19-297">En primer lugar, echemos un vistazo a la estructura de carpetas de los datos de texturas de prendas de ropa.</span><span class="sxs-lookup"><span data-stu-id="6aa19-297">En primer lugar, echemos un vistazo a la estructura de carpetas de los datos de texturas de prendas de ropa.</span></span> <span data-ttu-id="6aa19-298">Observe cómo todas las imágenes relativas a cada atributo se encuentran en las subcarpetas correspondientes (*dotted*, \*leopard y *striped*) dentro de *DATA_DIR/images/fashionTexture/*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-298">Observe cómo todas las imágenes relativas a cada atributo se encuentran en las subcarpetas correspondientes (*dotted*, \*leopard y *striped*) dentro de *DATA_DIR/images/fashionTexture/*.</span></span> <span data-ttu-id="6aa19-299">Observe también cómo el nombre de la carpeta de imágenes también aparece reflejado en el archivo `PARAMETERS.py`:</span><span class="sxs-lookup"><span data-stu-id="6aa19-299">Observe también cómo el nombre de la carpeta de imágenes también aparece reflejado en el archivo `PARAMETERS.py`:</span></span>
```python
datasetName = "fashionTexture"
```

<span data-ttu-id="6aa19-300">Usar un conjunto de datos personalizado es tan sencillo como reproducir esta misma estructura de carpetas donde están todas las imágenes en subcarpetas según sus atributos y, luego, copiar estas subcarpetas en un nuevo directorio especificado por el usuario, *DATA_DIR/images/newDataSetName/*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-300">Usar un conjunto de datos personalizado es tan sencillo como reproducir esta misma estructura de carpetas donde están todas las imágenes en subcarpetas según sus atributos y, luego, copiar estas subcarpetas en un nuevo directorio especificado por el usuario, *DATA_DIR/images/newDataSetName/*.</span></span> <span data-ttu-id="6aa19-301">El único cambio de código necesario consiste en establecer la variable `datasetName` en *newDataSetName*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-301">El único cambio de código necesario consiste en establecer la variable `datasetName` en *newDataSetName*.</span></span> <span data-ttu-id="6aa19-302">Tras ello, los scripts 1-5 se pueden ejecutar en orden y todos los archivos intermedios se escriben en *DATA_DIR/proc/newDataSetName/*.</span><span class="sxs-lookup"><span data-stu-id="6aa19-302">Tras ello, los scripts 1-5 se pueden ejecutar en orden y todos los archivos intermedios se escriben en *DATA_DIR/proc/newDataSetName/*.</span></span> <span data-ttu-id="6aa19-303">No se requiere ningún cambio de código más.</span><span class="sxs-lookup"><span data-stu-id="6aa19-303">No se requiere ningún cambio de código más.</span></span>

<span data-ttu-id="6aa19-304">Es importante que cada imagen pueda asignarse a exactamente un solo atributo.</span><span class="sxs-lookup"><span data-stu-id="6aa19-304">Es importante que cada imagen pueda asignarse a exactamente un solo atributo.</span></span> <span data-ttu-id="6aa19-305">Por ejemplo, sería incorrecto usar atributos para "animal" y para "leopardo", dado que "leopardo" también pertenecería a la clasificación "animal".</span><span class="sxs-lookup"><span data-stu-id="6aa19-305">Por ejemplo, sería incorrecto usar atributos para "animal" y para "leopardo", dado que "leopardo" también pertenecería a la clasificación "animal".</span></span> <span data-ttu-id="6aa19-306">De igual modo, conviene eliminar las imágenes ambiguas (y, por tanto, difíciles de anotar).</span><span class="sxs-lookup"><span data-stu-id="6aa19-306">De igual modo, conviene eliminar las imágenes ambiguas (y, por tanto, difíciles de anotar).</span></span>



### <a name="image-scraping-and-annotation"></a><span data-ttu-id="6aa19-307">Extracción y anotación de imágenes</span><span class="sxs-lookup"><span data-stu-id="6aa19-307">Extracción y anotación de imágenes</span></span>

<span data-ttu-id="6aa19-308">Recopilar un número suficientemente grande de imágenes anotadas para el entrenamiento y las pruebas puede resultar complicado.</span><span class="sxs-lookup"><span data-stu-id="6aa19-308">Recopilar un número suficientemente grande de imágenes anotadas para el entrenamiento y las pruebas puede resultar complicado.</span></span> <span data-ttu-id="6aa19-309">Una forma de solucionar este problema consiste en extraer imágenes de Internet.</span><span class="sxs-lookup"><span data-stu-id="6aa19-309">Una forma de solucionar este problema consiste en extraer imágenes de Internet.</span></span> <span data-ttu-id="6aa19-310">Por ejemplo, eche un vistazo a los siguientes resultados de una búsqueda de imágenes de Bing a raíz de la consulta *t-shirt striped* para encontrar camisetas a rayas.</span><span class="sxs-lookup"><span data-stu-id="6aa19-310">Por ejemplo, eche un vistazo a los siguientes resultados de una búsqueda de imágenes de Bing a raíz de la consulta *t-shirt striped* para encontrar camisetas a rayas.</span></span> <span data-ttu-id="6aa19-311">Como se esperaba, la mayoría de las imágenes son verdaderamente camisetas a rayas.</span><span class="sxs-lookup"><span data-stu-id="6aa19-311">Como se esperaba, la mayoría de las imágenes son verdaderamente camisetas a rayas.</span></span> <span data-ttu-id="6aa19-312">Las pocas imágenes incorrectas o ambiguas (por ejemplo, la de la columna 1, fila 1, o la de la columna 3, fila 2) se pueden identificar y quitar fácilmente:</span><span class="sxs-lookup"><span data-stu-id="6aa19-312">Las pocas imágenes incorrectas o ambiguas (por ejemplo, la de la columna 1, fila 1, o la de la columna 3, fila 2) se pueden identificar y quitar fácilmente:</span></span>
<p align="center">
<img src="media/scenario-image-classification-using-cntk/bing_search_striped.jpg" alt="alt text" width="600"/>
</p>

<span data-ttu-id="6aa19-313">Para generar un conjunto de datos voluminoso y diverso, hay que realizar varias consultas.</span><span class="sxs-lookup"><span data-stu-id="6aa19-313">Para generar un conjunto de datos voluminoso y diverso, hay que realizar varias consultas.</span></span> <span data-ttu-id="6aa19-314">Por ejemplo, si tenemos 7 \* 3 = 21 consultas, se pueden sintetizar automáticamente usando todas las combinaciones de prendas de ropa {blusa, sudadera, sudadera con capucha, jersey, camisa, camiseta, chaleco} y atributos {rayas, lunares, leopardo}.</span><span class="sxs-lookup"><span data-stu-id="6aa19-314">Por ejemplo, si tenemos 7 \* 3 = 21 consultas, se pueden sintetizar automáticamente usando todas las combinaciones de prendas de ropa {blusa, sudadera, sudadera con capucha, jersey, camisa, camiseta, chaleco} y atributos {rayas, lunares, leopardo}.</span></span> <span data-ttu-id="6aa19-315">Descargar las primeras 50 imágenes de cada consulta nos llevaría a tener un máximo de 21 x 50 = 1050 imágenes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-315">Descargar las primeras 50 imágenes de cada consulta nos llevaría a tener un máximo de 21 x 50 = 1050 imágenes.</span></span>

<span data-ttu-id="6aa19-316">En lugar de descargarlas manualmente desde la búsqueda de imágenes de Bing, es mucho más fácil usar en su lugar la [Bing Image Search API de Cognitive Services](https://www.microsoft.com/cognitive-services/bing-image-search-api), que devuelve un conjunto de direcciones URL de imágenes correspondientes a una cadena de consulta determinada.</span><span class="sxs-lookup"><span data-stu-id="6aa19-316">En lugar de descargarlas manualmente desde la búsqueda de imágenes de Bing, es mucho más fácil usar en su lugar la [Bing Image Search API de Cognitive Services](https://www.microsoft.com/cognitive-services/bing-image-search-api), que devuelve un conjunto de direcciones URL de imágenes correspondientes a una cadena de consulta determinada.</span></span>

<span data-ttu-id="6aa19-317">Algunas de las imágenes descargadas son duplicados exactos o casi exactos (por ejemplo, solo se diferencian por la resolución de imagen o por anomalías del formato jpg).</span><span class="sxs-lookup"><span data-stu-id="6aa19-317">Algunas de las imágenes descargadas son duplicados exactos o casi exactos (por ejemplo, solo se diferencian por la resolución de imagen o por anomalías del formato jpg).</span></span> <span data-ttu-id="6aa19-318">Estos duplicados se deben quitar para que los conjuntos de entrenamiento y de prueba no contengan las mismas imágenes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-318">Estos duplicados se deben quitar para que los conjuntos de entrenamiento y de prueba no contengan las mismas imágenes.</span></span> <span data-ttu-id="6aa19-319">La eliminación de imágenes duplicadas puede lograrse con un método basado en hash, consistente en dos pasos: (1) en primer lugar, se calcula la cadena hash de todas las imágenes y (2) en un segundo procesamiento de las imágenes, solamente se conservan aquellas que tengan una cadena hash que aún no se haya visto.</span><span class="sxs-lookup"><span data-stu-id="6aa19-319">La eliminación de imágenes duplicadas puede lograrse con un método basado en hash, consistente en dos pasos: (1) en primer lugar, se calcula la cadena hash de todas las imágenes y (2) en un segundo procesamiento de las imágenes, solamente se conservan aquellas que tengan una cadena hash que aún no se haya visto.</span></span> <span data-ttu-id="6aa19-320">Todas las demás imágenes se descartan.</span><span class="sxs-lookup"><span data-stu-id="6aa19-320">Todas las demás imágenes se descartan.</span></span> <span data-ttu-id="6aa19-321">Hemos constatado que el método `dhash` de la biblioteca de Python `imagehash` (descrito en este [blog](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html)) funciona bien con el parámetro `hash_size` establecido en 16.</span><span class="sxs-lookup"><span data-stu-id="6aa19-321">Hemos constatado que el método `dhash` de la biblioteca de Python `imagehash` (descrito en este [blog](http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html)) funciona bien con el parámetro `hash_size` establecido en 16.</span></span> <span data-ttu-id="6aa19-322">No pasa nada si se quitan algunas imágenes no duplicadas por error, siempre y cuando se quiten la mayoría de los duplicados reales.</span><span class="sxs-lookup"><span data-stu-id="6aa19-322">No pasa nada si se quitan algunas imágenes no duplicadas por error, siempre y cuando se quiten la mayoría de los duplicados reales.</span></span>





## <a name="conclusion"></a><span data-ttu-id="6aa19-323">Conclusión</span><span class="sxs-lookup"><span data-stu-id="6aa19-323">Conclusión</span></span>

<span data-ttu-id="6aa19-324">Algunos aspectos destacados de este ejemplo son:</span><span class="sxs-lookup"><span data-stu-id="6aa19-324">Algunos aspectos destacados de este ejemplo son:</span></span>
- <span data-ttu-id="6aa19-325">El código usado para entrenar, evaluar e implementar un modelo personalizado de clasificación de imágenes.</span><span class="sxs-lookup"><span data-stu-id="6aa19-325">El código usado para entrenar, evaluar e implementar un modelo personalizado de clasificación de imágenes.</span></span>
- <span data-ttu-id="6aa19-326">El uso de las imágenes de demostración suministradas, aunque esto es fácilmente adaptable (solo habría que cambiar una línea) para usar un conjunto de datos de imágenes propio.</span><span class="sxs-lookup"><span data-stu-id="6aa19-326">El uso de las imágenes de demostración suministradas, aunque esto es fácilmente adaptable (solo habría que cambiar una línea) para usar un conjunto de datos de imágenes propio.</span></span>
- <span data-ttu-id="6aa19-327">Las características modernas y de nivel de experto implementadas para entrenar modelos de alta precisión basados en el aprendizaje de transferencia.</span><span class="sxs-lookup"><span data-stu-id="6aa19-327">Las características modernas y de nivel de experto implementadas para entrenar modelos de alta precisión basados en el aprendizaje de transferencia.</span></span>
- <span data-ttu-id="6aa19-328">El desarrollo de modelos interactivos con Azure Machine Learning Workbench y Jupyter Notebook.</span><span class="sxs-lookup"><span data-stu-id="6aa19-328">El desarrollo de modelos interactivos con Azure Machine Learning Workbench y Jupyter Notebook.</span></span>


## <a name="references"></a><span data-ttu-id="6aa19-329">Referencias</span><span class="sxs-lookup"><span data-stu-id="6aa19-329">Referencias</span></span>

<span data-ttu-id="6aa19-330">[1] Alex Krizhevsky, Ilya Sutskever y Geoffrey E. Hinton.</span><span class="sxs-lookup"><span data-stu-id="6aa19-330">[1] Alex Krizhevsky, Ilya Sutskever y Geoffrey E. Hinton.</span></span> <span data-ttu-id="6aa19-331">[_ImageNet Classification with Deep Convolutional Neural Networks_](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (Clasificación de ImageNet con redes neuronales profundas de convolución).</span><span class="sxs-lookup"><span data-stu-id="6aa19-331">[_ImageNet Classification with Deep Convolutional Neural Networks_](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (Clasificación de ImageNet con redes neuronales profundas de convolución).</span></span> <span data-ttu-id="6aa19-332">NIPS 2012.</span><span class="sxs-lookup"><span data-stu-id="6aa19-332">NIPS 2012.</span></span>  
<span data-ttu-id="6aa19-333">[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren y Jian Sun.</span><span class="sxs-lookup"><span data-stu-id="6aa19-333">[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren y Jian Sun.</span></span> <span data-ttu-id="6aa19-334">[_Deep Residual Learning for Image Recognition_](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) (Aprendizaje residual profundo para el reconocimiento de imágenes).</span><span class="sxs-lookup"><span data-stu-id="6aa19-334">[_Deep Residual Learning for Image Recognition_](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) (Aprendizaje residual profundo para el reconocimiento de imágenes).</span></span> <span data-ttu-id="6aa19-335">CVPR 2016.</span><span class="sxs-lookup"><span data-stu-id="6aa19-335">CVPR 2016.</span></span>
